{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and link filings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccajohnson/opt/anaconda3/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from tabula import read_pdf\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "## profiling\n",
    "import time\n",
    "\n",
    "## plotting\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "\n",
    "## first, clean case type\n",
    "def process_type(one_row):\n",
    "    \n",
    "    ## some dates so convert to string\n",
    "    one_string = str(one_row)\n",
    "    \n",
    "    ## clean for expedited discipline\n",
    "    clean_exp_1 = re.sub(r'(Exped(i)?(t)?(e)?|Discip)\\s+', r'\\1', one_string)\n",
    "    clean_exp_2 = re.sub(r'(Exped(i)?(t)?(e)?|Discip)\\s+', r'\\1', clean_exp_1)\n",
    "    \n",
    "    ## clean for lea\n",
    "    clean_lea = re.sub(r'(Aga(i)?(n)?)\\s+', r'\\1', clean_exp_2)\n",
    "    clean_lea_ret = clean_lea.lower()\n",
    "    \n",
    "    return(clean_lea_ret)\n",
    "\n",
    "def process_schoolname(one_name):\n",
    "    \n",
    "    ## uppercase\n",
    "    name_str = str(one_name)\n",
    "    name_upper = name_str.upper()\n",
    "\n",
    "    ## clean up schools\n",
    "    clean_school= re.sub(r'(SCHOO)\\s+', r'\\1', name_upper)\n",
    "    clean_middle = re.sub(r'(MIDD)\\s+', r'\\1', clean_school)\n",
    "    clean_ed = re.sub(r'(EDUCAT)\\s+', r'\\1', clean_middle)\n",
    "    \n",
    "    ## concat whitespace\n",
    "    replace_middle = re.sub(r'M(\\s)?I(\\s)?D(\\s)?D(\\s)?L(\\s)?E', r\"MIDDLE\", clean_ed)\n",
    "    replace_elem = re.sub(r'E(\\s)?L(\\s)?E(\\s)?M(\\s)?E(\\s)?N(\\s)?T(\\s)?A(\\s)?R(\\s)?Y', r\"ELEMENTARY\", replace_middle)\n",
    "    replace_school = re.sub(r'SCHOOI', \"SCHOOL\", replace_elem)\n",
    "    replace_campus = re.sub(r'C(\\s)?A(\\s)?M(\\s)?P(\\s)?U(\\s)?S', r\"CAMPUS\", replace_school)\n",
    "    replace_education = re.sub(r'E(\\s)?D(\\s)?U(\\s)?C(\\s)?A(\\s)?T(\\s)?I(\\s)?O(\\s)?N', r\"EDUCATION\", \n",
    "                               replace_campus)\n",
    "    \n",
    "    ## \n",
    "\n",
    "    return(replace_education)\n",
    "\n",
    "def replace_schooltype(one_string):\n",
    "    \n",
    "    es = re.sub(r'ES$|ELEMENTARY$', r'ELEMENTARY SCHOOL', one_string)\n",
    "    ec = re.sub(r'EC$', r'ELEMENTARY CAMPUS', es)\n",
    "    ms = re.sub(r'MS$|MIDDLE$', r'MIDDLE SCHOOL', ec)\n",
    "    hs = re.sub(r'HS$|HIGH$', r'HIGH SCHOOL', ms)\n",
    "    \n",
    "    return(hs)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def find_fuzzy_namematches(one_name: str, all_names: list, \n",
    "                           score_cutoff):\n",
    "    \n",
    "    ## extract matches above cutoff\n",
    "    all_abovecutoff = process.extractBests(one_name, all_names, score_cutoff = score_cutoff,\n",
    "                                          limit = 1)\n",
    "    \n",
    "    ## make into a dataframe (will thus only capture ones with matches)\n",
    "    all_abovecutoff_df = pd.DataFrame(list(all_abovecutoff), columns = ['matched_name', 'score'])\n",
    "    all_abovecutoff_df['original_name'] = one_name\n",
    "    return(all_abovecutoff_df)\n",
    "\n",
    "## resource-- package installation issues: https://bergvca.github.io/2017/10/14/super-fast-string-matching.html\n",
    "\n",
    "\n",
    "\n",
    "def replace_missing_nces(one_val):\n",
    "    \n",
    "    if one_val.isdigit():\n",
    "        final_val = one_val\n",
    "    else:\n",
    "        final_val = np.nan\n",
    "    return(final_val)\n",
    "\n",
    "def aggregate_crdc(var_col, value_col, \n",
    "                  data,\n",
    "                  id_col = \"school_name\",\n",
    "                  year_chosen = 2013, format = \"long\"):\n",
    "    \n",
    "    \n",
    "    ## shape from long to wide\n",
    "    if(format == \"long\"):\n",
    "        df_wide = pd.pivot_table(data.loc[data.year == year_chosen,\n",
    "                    [id_col,\n",
    "                    var_col, \n",
    "                    value_col]],\n",
    "                    index  = id_col, \n",
    "                    values = value_col,\n",
    "                    columns = var_col).reset_index()\n",
    "\n",
    "    \n",
    "    else:\n",
    "        df_wide = data.loc[data.year == year_chosen].copy()\n",
    "        \n",
    "    ## standardize columns\n",
    "    df_wide.columns = [re.sub(\"\\s+\", \"_\", col.upper()) \n",
    "                           for col in df_wide.columns]\n",
    "        \n",
    "    ## generate rates\n",
    "    sub_cols = set(df_wide.columns).difference([\"SCHOOL_NAME\", \"TOTAL\", 'YEAR'])\n",
    "    \n",
    "    ## \n",
    "    for col in sub_cols:\n",
    "        df_wide[col] = pd.to_numeric(df_wide[col])\n",
    "        df_wide['TOTAL'] = pd.to_numeric(df_wide['TOTAL'])\n",
    "        df_wide['{}_rate'.format(col)] = df_wide[col]/df_wide['TOTAL']\n",
    "    \n",
    "    ## return\n",
    "    return(df_wide)\n",
    "    \n",
    "def aggregate_nces(var_pattern, varname_clean, id_col, \n",
    "                       cc_data_merged,\n",
    "                      enrollment_vars, base_name_raw):\n",
    "    \n",
    "    dem_vars = [col for col in cc_data_merged if var_pattern in col]\n",
    "    long_df = pd.melt(cc_data_merged[dem_vars + enrollment_vars + id_col],\n",
    "                       id_vars = id_col)\n",
    "    \n",
    "    ## create year versus dem col\n",
    "    long_df['clean_value_1'] = [replace_missing_nces(val) for val in long_df.value]\n",
    "    long_df['clean_value'] = pd.to_numeric(long_df.clean_value_1)\n",
    "    long_df['which_var'] = long_df.variable.astype(str).str.replace(\"\\\\_20.*\", \"\", regex = True)\n",
    "    replace_pattern = \"|\".join(long_df.which_var.unique())\n",
    "    long_df['which_year'] = [re.sub(replace_pattern, \"\", one_var) for one_var in long_df.variable]\n",
    "    long_toagg = long_df[id_col + ['which_var', 'which_year',\n",
    "                                           'clean_value']].reset_index()\n",
    "\n",
    "    ## do the aggregation \n",
    "    index_cols = id_col + [\"which_year\"]\n",
    "    df_wide = long_toagg.pivot_table(index  = index_cols,\n",
    "                                             values = 'clean_value',\n",
    "                                             columns = 'which_var').reset_index()\n",
    "\n",
    "    ## do the division\n",
    "    rate_varname = varname_clean + '_rate'\n",
    "    df_wide[rate_varname] = df_wide[var_pattern]/df_wide[base_name_raw]\n",
    "    #print(df_wide.head())\n",
    "    \n",
    "    ## return cleaned data\n",
    "    return(df_wide[id_col + [rate_varname] + ['which_year']])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and do prelim cleaning of filings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_filed</th>\n",
       "      <th>case_no</th>\n",
       "      <th>stud_homeschool</th>\n",
       "      <th>stud_attendschool</th>\n",
       "      <th>case_type</th>\n",
       "      <th>issue_case_type</th>\n",
       "      <th>against_osse_name</th>\n",
       "      <th>against_dcps_name</th>\n",
       "      <th>against_charter_name</th>\n",
       "      <th>type_closingorder</th>\n",
       "      <th>date_closed_orderissued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>2012-0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Prospect LC</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prospect LC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Order</td>\n",
       "      <td>1/27/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>2012-0003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsie Whitlow Stokes Community Freedom PCS</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Order</td>\n",
       "      <td>1/19/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>2012-0006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cesar Chavez PCS</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cesar Chavez PCS</td>\n",
       "      <td>Order</td>\n",
       "      <td>5/8/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>2012-0005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Columbia Heights Education Center High School</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Columbia Heights Education Center High School</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Order</td>\n",
       "      <td>2/28/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>2012-0007</td>\n",
       "      <td>Johnson MS</td>\n",
       "      <td>Johnson MS</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>Against LEA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Johnson MS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOD</td>\n",
       "      <td>1/20/2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_filed    case_no stud_homeschool  \\\n",
       "0   1/3/2012  2012-0002             NaN   \n",
       "1   1/3/2012  2012-0003             NaN   \n",
       "2   1/4/2012  2012-0006             NaN   \n",
       "3   1/4/2012  2012-0005             NaN   \n",
       "4   1/5/2012  2012-0007      Johnson MS   \n",
       "\n",
       "                               stud_attendschool    case_type issue_case_type  \\\n",
       "0                                    Prospect LC  Against LEA     Against LEA   \n",
       "1     Elsie Whitlow Stokes Community Freedom PCS  Against LEA     Against LEA   \n",
       "2                               Cesar Chavez PCS  Against LEA     Against LEA   \n",
       "3  Columbia Heights Education Center High School  Against LEA     Against LEA   \n",
       "4                                     Johnson MS  Against LEA     Against LEA   \n",
       "\n",
       "  against_osse_name                              against_dcps_name  \\\n",
       "0               NaN                                    prospect LC   \n",
       "1               NaN                                            NaN   \n",
       "2               NaN                                            NaN   \n",
       "3               NaN  Columbia Heights Education Center High School   \n",
       "4               NaN                                     Johnson MS   \n",
       "\n",
       "  against_charter_name type_closingorder date_closed_orderissued  \n",
       "0                  NaN             Order               1/27/2012  \n",
       "1                  NaN             Order               1/19/2012  \n",
       "2     Cesar Chavez PCS             Order                5/8/2012  \n",
       "3                  NaN             Order               2/28/2012  \n",
       "4                  NaN               HOD               1/20/2012  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_filings = pd.read_excel(\"../../raw_input/dc/dc_alternate_extraction.xlsx\")\n",
    "dc_filings.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cleaning case type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "against lea                    3117\n",
       "expedited discipline            272\n",
       "nan                              61\n",
       "expedited non discipline         50\n",
       "by lea                           30\n",
       "aga•nst lea                      14\n",
       "against sea                      10\n",
       "expedited non                     5\n",
       "agailea                           5\n",
       "expedited discipline by lea       1\n",
       "expedited non discioline          1\n",
       "Name: casetype_clean_init, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dc_filings['casetype_clean_init'] = [process_type(one_type) for one_type in dc_filings.case_type.tolist()]\n",
    "dc_filings.casetype_clean_init.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_filings['casetype_isdigits'] = [\"digits\" if re.match(r'[0-9]+', one_str) is not None  else \"no_digits\" \n",
    "        for one_str in dc_filings.casetype_clean_init]\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Against LEA                 3136\n",
       "Expedited Discipline         273\n",
       "Other/failed to parse         61\n",
       "Expedited non-discipline      56\n",
       "Against SEA                   30\n",
       "By LEA                        10\n",
       "Name: derived_casetype_final, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casetypes_conds = [dc_filings.casetype_clean_init.str.contains(\"expedited non\"),\n",
    "                   (dc_filings.casetype_clean_init.str.contains(\"discipline\")) | \n",
    "                   (dc_filings.casetype_clean_init.str.contains(\"expedited disc\")),\n",
    "                   (dc_filings.casetype_clean_init.str.contains(\"lea\")) & \n",
    "                    (dc_filings.casetype_clean_init != \"by lea\"),\n",
    "                   dc_filings.casetype_clean_init == \"by lea\",\n",
    "                   dc_filings.casetype_clean_init.str.contains(\"against se\")]\n",
    "casetypes_codeto = ['Expedited non-discipline', 'Expedited Discipline', 'Against LEA', 'Against SEA', \n",
    "                   'By LEA']\n",
    "\n",
    "assert len(casetypes_conds) == len(casetypes_codeto)\n",
    "\n",
    "## apply\n",
    "dc_filings['derived_casetype_final'] = np.select(casetypes_conds,\n",
    "                                        casetypes_codeto,\n",
    "                                        default= 'Other/failed to parse')\n",
    "dc_filings.derived_casetype_final.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_filings['date_filed_temp'] = np.where(dc_filings.case_no == \"2015-0083\", \n",
    "                                          \"03/11/2015\",\n",
    "                                np.where(dc_filings.case_no == \"2016-0287\",\n",
    "                                            \"12/06/2016\",\n",
    "                                           dc_filings.date_filed)) # manually fixing 1 where month got cutoff\n",
    "dc_filings['derived_date_filed'] = pd.to_datetime(dc_filings.date_filed_temp, errors = \"coerce\")\n",
    "\n",
    "dc_filings['derived_date_closed'] = pd.to_datetime(dc_filings.date_closed_orderissued, errors = \"coerce\")\n",
    "\n",
    "\n",
    "## construct indicator for duration\n",
    "dc_filings['derived_daysbt_fileclosed'] = np.where(~dc_filings.derived_date_closed.isnull(),\n",
    "                (dc_filings['derived_date_closed'] - dc_filings['derived_date_filed']).dt.days,\n",
    "                np.nan)\n",
    "#dc_filings.derived_daysbt_fileclosed.value_counts()\n",
    "\n",
    "### maybe add business day counts and school years - np.busday_count('2018-04-10', '2018-04-11') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge in district demographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create name-nces ID crosswalk using common core data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 75-col limit in export-- first 75 cols\n",
    "cc_data_1 = pd.read_csv(\"../../raw_input/dc/dc_ccd.csv\")\n",
    "\n",
    "## \n",
    "cc_data_2 = pd.read_csv(\"../../raw_input/dc/dc_ccd_pull2.csv\",\n",
    "                       encoding= 'unicode_escape')\n",
    "\n",
    "## find overlapping cols\n",
    "cc_data_1_topull = cc_data_1.columns.difference(cc_data_2.columns).tolist() + \\\n",
    "                    [\"School Name\", \n",
    "                     \"School ID - NCES Assigned [Public School] Latest available year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge excluding\n",
    "cc_data_merged = pd.merge(cc_data_1[cc_data_1_topull], \n",
    "                          cc_data_2[[col for col in cc_data_2.columns if \n",
    "                            \"School ID - NCES Assigned [Public School] Latest available year\" not in col]], \n",
    "                          on = [\"School Name\"],\n",
    "                         how = \"left\")\n",
    "\n",
    "#cc_data_merged[[col for col in cc_data_merged.columns if 'ID' in col] + ['School Name']].sample(n = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_cleancols = [re.sub(\"\\\\s+|\\\\[|\\\\]|\\\\(|\\\\)\", \"_\", x).upper() for x in cc_data_merged.columns]\n",
    "cc_data_merged.columns = cc_cleancols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create crosswalk to do matching\n",
    "school_id = 'SCHOOL_ID_-_NCES_ASSIGNED__PUBLIC_SCHOOL__LATEST_AVAILABLE_YEAR'\n",
    "cc_crosswalk = cc_data_merged[['SCHOOL_NAME', \n",
    "                        school_id]].copy().drop_duplicates()\n",
    "\n",
    "\n",
    "cc_crosswalk['name_tocompare_commonc'] = [replace_schooltype(one_school) for one_school in cc_crosswalk.SCHOOL_NAME]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create name-nces id crosswalk using CRDC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try looking in CRDC--- create similar crosswalk; exact and non\n",
    "\n",
    "crdc_largerpull = pd.read_csv(\"../../raw_input/dc/EducationDataPortal_03.08.2020_schools.csv\")\n",
    "crdc_crosswalk = crdc_largerpull[['school_name', 'ncessch']].copy().drop_duplicates()\n",
    "crdc_crosswalk['school_name'] = crdc_crosswalk.school_name.str.upper()\n",
    "\n",
    "## clean up name in similar way\n",
    "crdc_crosswalk['name_tocompare_crdc'] = [replace_schooltype(one_school) \n",
    "                                 for one_school in crdc_crosswalk.school_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clean school name in filings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_filings['derived_schoolagainst'] = np.where(~dc_filings.against_dcps_name.isnull(),\n",
    "                                              dc_filings.against_dcps_name,\n",
    "                                        np.where(~dc_filings.against_charter_name.isnull(),\n",
    "                                                dc_filings.against_charter_name,\n",
    "                                         np.where(~dc_filings.stud_attendschool.isnull(),\n",
    "                                                 dc_filings.stud_attendschool,\n",
    "                                                 np.nan)))\n",
    "dc_filings['school_against_cleaned_1'] = [process_schoolname(one_name) \n",
    "                                                for one_name in dc_filings.derived_schoolagainst]\n",
    "dc_filings['school_against_cleaned'] = [replace_schooltype(one_name)\n",
    "                                               for one_name in dc_filings.school_against_cleaned_1]\n",
    "# dc_filings[dc_filings.derived_schoolagainst.isnull()] - to return to\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Try exact matches\n",
    "\n",
    "Hierarchical where we first try CRDC, then try common core, since the former has more granular school ids than latter (which codes all DCPS schools to same id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Both exact        0.498\n",
       "Neither exact     0.450\n",
       "CRDC exact only   0.038\n",
       "CCD exact only    0.014\n",
       "Name: match_cat, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## first, try exact match with crdc\n",
    "dc_filings_exact = pd.merge(dc_filings[~dc_filings.derived_schoolagainst.isnull()].copy(),\n",
    "                           crdc_crosswalk,\n",
    "                            left_on = 'school_against_cleaned',\n",
    "                            right_on = 'name_tocompare_crdc',\n",
    "                           how = \"left\",\n",
    "                           indicator = \"exactm_crdc\")\n",
    "\n",
    "## then, try exact match of those with ccd\n",
    "dc_filings_exact_c = pd.merge(dc_filings_exact,\n",
    "                           cc_crosswalk,\n",
    "                            left_on = 'school_against_cleaned',\n",
    "                            right_on = 'name_tocompare_commonc',\n",
    "                           how = \"left\",\n",
    "                           indicator = \"exactm_ccd\",\n",
    "                        suffixes = [\"_crdc\", \"_ccd\"])\n",
    "\n",
    "## code categories\n",
    "dc_filings_exact_c['match_cat'] = np.where((dc_filings_exact_c.exactm_crdc == \"both\") & \n",
    "                                           (dc_filings_exact_c.exactm_ccd == \"both\"),\n",
    "                                        \"Both exact\",\n",
    "                                np.where((dc_filings_exact_c.exactm_ccd == \"both\"), \n",
    "                                        \"CCD exact only\",\n",
    "                                np.where(dc_filings_exact_c.exactm_crdc == \"both\",\n",
    "                                        \"CRDC exact only\",\n",
    "                                        \"Neither exact\")))\n",
    "\n",
    "dc_filings_exact_c.match_cat.value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2587 cases exact; 2090 need fuzzy; 25 other'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## separate into three categories:\n",
    "## 1. exact match\n",
    "## 2. not exact match but needs it\n",
    "## 3. not applicable\n",
    "filings_exactm = dc_filings_exact_c.loc[dc_filings_exact_c.match_cat != \"Neither exact\"].copy()\n",
    "filings_needfuzzy = dc_filings_exact_c.loc[(~dc_filings_exact_c.school_against_cleaned.isnull()) &\n",
    "                                   (dc_filings_exact_c.match_cat ==  \"Neither exact\") &\n",
    "                                    (~dc_filings_exact_c.derived_casetype_final.isin(['Against SEA', 'By LEA']))]\n",
    "caseno_tomatch = filings_exactm.case_no.to_list() + filings_needfuzzy.case_no.to_list()\n",
    "filings_other = dc_filings_exact_c[~dc_filings_exact_c.case_no.isin(caseno_tomatch)].copy()\n",
    "\"\"\"{} cases exact; {} need fuzzy; {} other\"\"\".format(filings_exactm.shape[0],\n",
    "                                                filings_needfuzzy.shape[0],\n",
    "                                                filings_other.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for ones need fuzzy, subset to names only\n",
    "filings_crosswalk = filings_needfuzzy[['school_against_cleaned']].drop_duplicates()\n",
    "filings_crosswalk['id'] = filings_crosswalk.index+1\n",
    "filings_crosswalk.shape\n",
    "\n",
    "\n",
    "## write to intermediate --- first try crdc versus filings; then common core\n",
    "crdc_crosswalk.to_csv(\"../../intermediate_objects/crdc_schoolnames_f22run.csv\")\n",
    "filings_crosswalk.to_csv(\"../../intermediate_objects/filings_names_f22run.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ran script: 03helper_fuzzymatch_nces.csv\n",
    "\n",
    "\n",
    "# cc_crosswalk.to_csv(\"../../intermediate_objects/nces_schoolnames_f22run.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 merge in fuzzy match results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 success via fuzzy match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results of fmatch\n",
    "fm_crdc_filings = pd.read_csv(\"../../intermediate_objects/filings_fuzzymatch_crdc_fall22.csv\")\n",
    "\n",
    "## subset to 90 and above\n",
    "fm_crdc_filings_keep = fm_crdc_filings[fm_crdc_filings.score >= 90].copy()\n",
    "\n",
    "## write the kept ones for RA to check\n",
    "fm_crdc_filings_keep['keep'] = 1\n",
    "fm_crdc_filings_keep['school_new'] = \"\"\n",
    "fm_crdc_filings_keep.sort_values(by = 'score', ascending = True).to_csv(\"../../filings_fuzzymatch_RAcheck.csv\",\n",
    "                                                                       index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge back using original name\n",
    "filings_crosswalk_wmatch = pd.merge(filings_crosswalk, \n",
    "                                   fm_crdc_filings_keep[['original_name', 'matched_name', 'score']],\n",
    "                                   left_on = 'school_against_cleaned',\n",
    "                                   right_on = 'original_name',\n",
    "                                   how = \"left\")\n",
    "\n",
    "filings_crosswalk_wmatch['matched'] = filings_crosswalk_wmatch.score.notnull()\n",
    "\n",
    "\n",
    "## separate into matched and nonmatched\n",
    "filings_matchfuzzy = filings_crosswalk_wmatch[filings_crosswalk_wmatch.matched].copy()\n",
    "filings_nonmatchfuzzy = filings_crosswalk_wmatch[~filings_crosswalk_wmatch.matched].sort_values(by = 'original_name')\n",
    "filings_nonmatchfuzzy.to_csv(\"../../intermediate_objects/dc_snames_nonmatch_f22.csv\",\n",
    "                            index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge in nces ID based on matched name\n",
    "filings_crosswalk_wid = pd.merge(filings_crosswalk_wmatch[filings_crosswalk_wmatch.matched],\n",
    "                                crdc_crosswalk.drop_duplicates(),\n",
    "                                 left_on = 'matched_name',\n",
    "                                 right_on = 'name_tocompare_crdc',\n",
    "                                 how = \"left\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 manual match \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in manual matches\n",
    "filings_manual = pd.read_csv(\"../../intermediate_objects/dc_snames_match_f22 - dc_snames_nonmatch_f22.csv\")\n",
    "filings_manual_matched = filings_manual[filings_manual.matched_name.notnull()].copy()\n",
    "filings_crosswalk_wid_manual = pd.merge(filings_manual_matched,\n",
    "                                crdc_crosswalk.drop_duplicates(),\n",
    "                                 left_on = 'matched_name',\n",
    "                                 right_on = 'name_tocompare_crdc',\n",
    "                                 how = \"left\").drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "## look at onces not matched\n",
    "filings_manual_notmatched = filings_manual[filings_manual.matched_name.isnull()].copy()\n",
    "\n",
    "## write the results\n",
    "#filings_towrite = filings_manual_notmatched[['school_against_cleaned', 'notes']]\n",
    "#filings_towrite['school'] = filings_towrite.school_against_cleaned.str.replace(\".*PG|-|•\", \"\")\n",
    "#filings_towrite['notes'] = np.where(filings_towrite.notes == \"Many schools\",\n",
    " #                                  \"Multiple campuses\", \n",
    "  #                                 filings_towrite.notes)\n",
    "#filings_distinct = filings_towrite[['school', 'notes']].drop_duplicates(subset = ['school']).sort_values(by = \n",
    "                   #                     'school')\n",
    "#filings_distinct = filings_distinct.fillna(\"\")\n",
    "#print(filings_distinct.to_latex(index = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separate ones that we tried to match into two categories:\n",
    "## (1) successfully matched or\n",
    "## (2) not\n",
    "matches_both = filings_crosswalk_wid_manual.school_against_cleaned.to_list() + \\\n",
    "                                    filings_crosswalk_wid.original_name.to_list()\n",
    "filings_needfuzzy_matched = filings_needfuzzy[filings_needfuzzy.school_against_cleaned.isin(\\\n",
    "                                            matches_both)].copy()\n",
    "filings_needfuzzy_notmatched = filings_needfuzzy[~filings_needfuzzy.school_against_cleaned.isin(\\\n",
    "                                            matches_both)].copy()\n",
    "\n",
    "\n",
    "filings_needfuzzy_tomerge = filings_needfuzzy_matched[[col for col in filings_needfuzzy_matched.columns\n",
    "                                             if col not in [\"ncessch\",\n",
    "                                            \"name_tocompare_crdc\"]]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filings_needfuzzy_merged = pd.merge(filings_needfuzzy_tomerge,\n",
    "                                   filings_crosswalk_wid[['name_tocompare_crdc',\n",
    "                                                         'ncessch', 'original_name']].copy(),\n",
    "                                   left_on = \"school_against_cleaned\",\n",
    "                                   right_on = \"original_name\",\n",
    "                                   how = \"left\",\n",
    "                                   indicator = \"fuzzym\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filings_needfuzzy_merged['match_cat'] = 'Fuzzy CRDC or manual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filings_daterange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9de3fbe3c921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#sorted(filings_matched_daterange.columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilings_matched_daterange\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase_no\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilings_daterange\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase_no\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilings_matched_daterange\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncessch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilings_matched_daterange\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncessch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filings_daterange' is not defined"
     ]
    }
   ],
   "source": [
    "## rowbind the two datasets:\n",
    "## 1. match exact\n",
    "## 2. match fuzzy\n",
    "## for the match neither, trying the other source \n",
    "overlap_cols = set(filings_exactm.columns).intersection(filings_needfuzzy_merged.columns)\n",
    "filings_exactfuzzy = pd.concat([filings_needfuzzy_merged[overlap_cols],\n",
    "                               filings_exactm[overlap_cols]]).drop_duplicates()\n",
    "\n",
    "\n",
    "## go back to filings and subset to the relevant school years - summarize match rates\n",
    "## SYs: 2014-2015 to 2017-2018\n",
    "## SY 2014 start date: https://dcps.dc.gov/sites/default/files/dc/sites/dcps/publication/attachments/REVISED%20School%20Year%202014-2015%20Full%20Calendar%20update.pdf\n",
    "## 08-25-2014\n",
    "sy_1415 = pd.to_datetime(\"2014-08-25\")\n",
    "sy_1718 = pd.to_datetime(\"2018-06-15\")\n",
    "\n",
    "dc_filings['in_date_range'] = np.where((dc_filings.derived_date_filed >= sy_1415) &\n",
    "                              (dc_filings.derived_date_filed <= sy_1718), True, False)\n",
    "filings_exactfuzzy['in_date_range'] = np.where((filings_exactfuzzy.derived_date_filed >= sy_1415) &\n",
    "                              (filings_exactfuzzy.derived_date_filed <= sy_1718), True, False)\n",
    "\n",
    "## check how many matched\n",
    "filings_matched_daterange = filings_exactfuzzy[filings_exactfuzzy.in_date_range].copy()\n",
    "\n",
    "\n",
    "## cols to keep\n",
    "cols_keep = ['case_no', 'type_closingorder', \n",
    "            'ncessch', 'school_against_cleaned', \n",
    "            'name_tocompare_commonc', 'name_tocompare_crdc',\n",
    "            'match_cat', 'SCHOOL_ID_-_NCES_ASSIGNED__PUBLIC_SCHOOL__LATEST_AVAILABLE_YEAR'] + [col for col in filings_matched_daterange.columns if \n",
    "                           \"derived\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "filings_matched_towrite = filings_matched_daterange[cols_keep].copy()\n",
    "filings_matched_towrite.to_csv(\"../../intermediate_objects/cleaned_df/dc_filings.csv\",\n",
    "                              index = False) \n",
    "filings_matched_towrite.to_pickle(\"../../intermediate_objects/cleaned_df/dc_filings.pkl\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

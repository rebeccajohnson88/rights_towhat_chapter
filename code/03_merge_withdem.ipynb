{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and link filings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabula import read_pdf\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "## profiling\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "## first, clean case type\n",
    "def process_type(one_row):\n",
    "    \n",
    "    ## some dates so convert to string\n",
    "    one_string = str(one_row)\n",
    "    \n",
    "    ## clean for expedited discipline\n",
    "    clean_exp_1 = re.sub(r'(Exped(i)?(t)?(e)?|Discip)\\s+', r'\\1', one_string)\n",
    "    clean_exp_2 = re.sub(r'(Exped(i)?(t)?(e)?|Discip)\\s+', r'\\1', clean_exp_1)\n",
    "    \n",
    "    ## clean for lea\n",
    "    clean_lea = re.sub(r'(Aga(i)?(n)?)\\s+', r'\\1', clean_exp_2)\n",
    "    \n",
    "    return(clean_lea)\n",
    "\n",
    "def process_schoolname(one_name):\n",
    "    \n",
    "    ## uppercase\n",
    "    name_str = str(one_name)\n",
    "    name_upper = name_str.upper()\n",
    "\n",
    "    ## clean up schools\n",
    "    clean_school= re.sub(r'(SCHOO)\\s+', r'\\1', name_upper)\n",
    "    clean_middle = re.sub(r'(MIDD)\\s+', r'\\1', clean_school)\n",
    "    clean_ed = re.sub(r'(EDUCAT)\\s+', r'\\1', clean_middle)\n",
    "    \n",
    "    ## concat whitespace\n",
    "    replace_middle = re.sub(r'M(\\s)?I(\\s)?D(\\s)?D(\\s)?L(\\s)?E', r\"MIDDLE\", clean_ed)\n",
    "    replace_elem = re.sub(r'E(\\s)?L(\\s)?E(\\s)?M(\\s)?E(\\s)?N(\\s)?T(\\s)?A(\\s)?R(\\s)?Y', r\"ELEMENTARY\", replace_middle)\n",
    "    replace_school = re.sub(r'SCHOOI', \"SCHOOL\", replace_elem)\n",
    "    replace_campus = re.sub(r'C(\\s)?A(\\s)?M(\\s)?P(\\s)?U(\\s)?S', r\"CAMPUS\", replace_school)\n",
    "    replace_education = re.sub(r'E(\\s)?D(\\s)?U(\\s)?C(\\s)?A(\\s)?T(\\s)?I(\\s)?O(\\s)?N', r\"EDUCATION\", \n",
    "                               replace_campus)\n",
    "    \n",
    "    ## \n",
    "\n",
    "    return(replace_education)\n",
    "\n",
    "def replace_schooltype(one_string):\n",
    "    \n",
    "    es = re.sub(r'ES$|ELEMENTARY$', r'ELEMENTARY SCHOOL', one_string)\n",
    "    ec = re.sub(r'EC$', r'ELEMENTARY CAMPUS', es)\n",
    "    ms = re.sub(r'MS$|MIDDLE$', r'MIDDLE SCHOOL', ec)\n",
    "    hs = re.sub(r'HS$|HIGH$', r'HIGH SCHOOL', ms)\n",
    "    \n",
    "    return(hs)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def find_fuzzy_namematches(one_name: str, all_names: list, \n",
    "                           score_cutoff):\n",
    "    \n",
    "    ## extract matches above cutoff\n",
    "    all_abovecutoff = process.extractBests(one_name, all_names, score_cutoff = score_cutoff,\n",
    "                                          limit = 1)\n",
    "    \n",
    "    ## make into a dataframe (will thus only capture ones with matches)\n",
    "    all_abovecutoff_df = pd.DataFrame(list(all_abovecutoff), columns = ['matched_name', 'score'])\n",
    "    all_abovecutoff_df['original_name'] = one_name\n",
    "    return(all_abovecutoff_df)\n",
    "\n",
    "## resource-- package installation issues: https://bergvca.github.io/2017/10/14/super-fast-string-matching.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try redoing the parsing to get\n",
    "## date closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and do prelim cleaning of filings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After removing those that failed to parse, go from 7949 filings to 7752 filings.\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_filings_init = pd.read_csv(\"../data/dc/intermediate/processed_filings.csv\")\n",
    "\n",
    "dc_filings_init['failed_parse'] = np.where(dc_filings_init.eq(dc_filings_init.iloc[:, 0], \n",
    "                                axis=0).all(1), 1, 0)\n",
    "\n",
    "\n",
    "## get row number of those that failed parse to reprocess\n",
    "rownums_failedparse = pd.DataFrame({'missing_info':\n",
    "                dc_filings_init.loc[dc_filings_init.failed_parse == 1].index.tolist()})\n",
    "\n",
    "## write those and go back to process tables, pulling all cols for those rows\n",
    "rownums_failedparse.to_pickle(\"../data/dc/intermediate/rownums_failedparse.pickle\")\n",
    "\n",
    "\n",
    "## subset to ones that parsed\n",
    "dc_filings = dc_filings_init.loc[dc_filings_init.failed_parse == 0, ].copy()\n",
    "\n",
    "\"\"\"After removing those that failed to parse, go from {} filings to {} filings.\n",
    "\"\"\".format(dc_filings_init.shape[0],\n",
    "          dc_filings.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012              1651\n",
       "2013              1459\n",
       "2014              1023\n",
       "2015              1001\n",
       "2018               834\n",
       "2017               702\n",
       "2016               626\n",
       "2019               269\n",
       "failed_toparse     187\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_filings['casetype_clean_init'] = [process_type(one_type) for one_type in dc_filings.casetype.tolist()]\n",
    "dc_filings['casetype_isdigits'] = [\"digits\" if re.match(r'[0-9]+', one_str) is not None  else \"no_digits\" \n",
    "        for one_str in dc_filings.casetype_clean_init]\n",
    "\n",
    "## by subsetting to those, see that year is still in the case so don't need to use for that\n",
    "dc_filings['casetype_final'] = np.where((dc_filings.casetype_clean_init.str.contains(\"Discip\")) |\n",
    "                                        (dc_filings.casetype_clean_init.str.contains(\"Expedited\")),\n",
    "                                        \"Expedited Discipline\",\n",
    "                                np.where((dc_filings.casetype_clean_init.str.contains(\"LEA\")) & \n",
    "                                         (dc_filings.casetype_clean_init != \"By LEA\"), \"Against LEA\",\n",
    "                                np.where(dc_filings.casetype_clean_init == \"By LEA\", \"By LEA\",\n",
    "                                np.where(dc_filings.casetype_clean_init.str.contains(\"Against SE\"),\n",
    "                                        \"Against SEA\",\n",
    "                                        \"Other/failed to parse\"))))\n",
    "\n",
    "\n",
    "## write the failed to parse ones\n",
    "## write those and go back to process tables, pulling the rows manually\n",
    "dc_filings[dc_filings.casetype_final == \"Other/failed to parse\"].to_csv(\"../data/dc/intermediate/missing_casetype.csv\")\n",
    "\n",
    "\n",
    "## get range of dates of the filings\n",
    "dc_filings['year_init'] = [re.sub(r'^(20[1-2][0-9]).*', r'\\1', str(one_string)) for one_string in \n",
    "                      dc_filings.case_no]\n",
    "year_range = [str(i) for i in np.arange(2012, 2020).tolist()]\n",
    "dc_filings['year'] = np.where(dc_filings.year_init.isin(year_range), dc_filings.year_init,\n",
    "                             'failed_toparse')\n",
    "dc_filings.year.value_counts() # half the year in 2019\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['attending_school', 'case_no', 'casetype', 'dcps_school_against',\n",
       "       'home_school', 'failed_parse', 'casetype_clean_init',\n",
       "       'casetype_isdigits', 'casetype_final', 'year_init', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_filings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load district demographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create name-nces ID crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = pd.read_csv(\"../data/dc/intermediate/dc_ccd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCHOOL_NAME</th>\n",
       "      <th>SCHOOL_ID_-_NCES_ASSIGNED__PUBLIC_SCHOOL__LATEST_AVAILABLE_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACADEMY OF HOPE ADULT PCS</td>\n",
       "      <td>110009000000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACHIEVEMENT PREPARATORY PCS ELEMENTARY</td>\n",
       "      <td>110007000000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACHIEVEMENT PREPARATORY PCS MIDDLE SCHOOL</td>\n",
       "      <td>110007000000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADAMS ELEMENTARY SCHOOL                       ...</td>\n",
       "      <td>110003000000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADVANCED PATH ACADEMY</td>\n",
       "      <td>110003000000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         SCHOOL_NAME  \\\n",
       "0                          ACADEMY OF HOPE ADULT PCS   \n",
       "1             ACHIEVEMENT PREPARATORY PCS ELEMENTARY   \n",
       "2          ACHIEVEMENT PREPARATORY PCS MIDDLE SCHOOL   \n",
       "3  ADAMS ELEMENTARY SCHOOL                       ...   \n",
       "4                              ADVANCED PATH ACADEMY   \n",
       "\n",
       "   SCHOOL_ID_-_NCES_ASSIGNED__PUBLIC_SCHOOL__LATEST_AVAILABLE_YEAR  \n",
       "0                                   110009000000.000                \n",
       "1                                   110007000000.000                \n",
       "2                                   110007000000.000                \n",
       "3                                   110003000000.000                \n",
       "4                                   110003000000.000                "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_cleancols = [re.sub(\"\\\\s+|\\\\[|\\\\]\", \"_\", x).upper() for x in cc_data.columns]\n",
    "cc_data.columns = cc_cleancols\n",
    "\n",
    "## create crosswalk to do matching\n",
    "cc_crosswalk = cc_data[['SCHOOL_NAME', \n",
    "                        'SCHOOL_ID_-_NCES_ASSIGNED__PUBLIC_SCHOOL__LATEST_AVAILABLE_YEAR']].copy()\n",
    "cc_crosswalk.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cc_crosswalk['name_tocompare'] = [replace_schooltype(one_school) for one_school in cc_crosswalk.SCHOOL_NAME]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clean school name and fuzzy matching to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess school to clean\n",
    "## and do fuzzy matching\n",
    "dc_filings_tomatch = dc_filings[['case_no', 'dcps_school_against', 'year']].drop_duplicates(subset = \n",
    "                                                ['case_no',\n",
    "                                                'dcps_school_against'])\n",
    "\n",
    "\n",
    "\n",
    "dc_filings_tomatch['school_against_cleaned_1'] = [process_schoolname(one_name) \n",
    "                                                for one_name in dc_filings_tomatch.dcps_school_against]\n",
    "dc_filings_tomatch['school_against_cleaned'] = [replace_schooltype(one_name)\n",
    "                                               for one_name in dc_filings_tomatch.school_against_cleaned_1]\n",
    "\n",
    "\n",
    "\n",
    "## generate tf-idf representation\n",
    "filings_crosswalk = dc_filings_tomatch[['school_against_cleaned']].drop_duplicates()\n",
    "filings_crosswalk['id'] = filings_crosswalk.index+1\n",
    "\n",
    "## \n",
    "\n",
    "subset_names = filings_crosswalk.school_against_cleaned.sample(20)\n",
    "\n",
    "\n",
    "## write to intermediate\n",
    "cc_crosswalk.to_csv(\"../data/dc/intermediate/nces_schoolnames.csv\")\n",
    "filings_crosswalk.to_csv(\"../data/dc/intermediate/filings_names.csv\")\n",
    "\n",
    "\n",
    "\n",
    "all_names = cc_crosswalk.name_tocompare.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LECKIE ELEMENTARY CAMPUS']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[school for school in all_names if \"LECKIE\" in school]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy matching took 9.110193252563477 seconds to run\n"
     ]
    }
   ],
   "source": [
    "fuzzymatch_results_list = [find_fuzzy_namematches(name, all_names, 90) \n",
    "                           for name in subset_names]\n",
    "t1 = time.time()\n",
    "print(\"Fuzzy matching took \" + str(t1-t0) + \" seconds to run\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_name</th>\n",
       "      <th>score</th>\n",
       "      <th>original_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RANDLE HIGHLANDS ELEMENTARY SCHOOL</td>\n",
       "      <td>97</td>\n",
       "      <td>RANDL E-HIGH LANDS ELEMENTARY SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HEARST ELEMENTARY SCHOOL</td>\n",
       "      <td>100</td>\n",
       "      <td>HEARST ELEMENTARY SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EATON ELEMENTARY SCHOOL</td>\n",
       "      <td>100</td>\n",
       "      <td>EATON ELEMENTARY SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KRAMER MIDDLE SCHOOL</td>\n",
       "      <td>98</td>\n",
       "      <td>KRAME R MIDDLE SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PHELPS ARCHITECTURE CONSTRUCTION AND ENGINEERI...</td>\n",
       "      <td>97</td>\n",
       "      <td>PHELPS ARCHITECTURE, CONSTRUCTION, AND ENG INE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ELIOT HINE MIDDLE SCHOOL</td>\n",
       "      <td>95</td>\n",
       "      <td>HINE MIDDLE SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POWELL ELEMENTARY SCHOOL</td>\n",
       "      <td>98</td>\n",
       "      <td>POWE LL ELEMENTARY SCHOOL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        matched_name score  \\\n",
       "0                 RANDLE HIGHLANDS ELEMENTARY SCHOOL    97   \n",
       "0                           HEARST ELEMENTARY SCHOOL   100   \n",
       "0                            EATON ELEMENTARY SCHOOL   100   \n",
       "0                               KRAMER MIDDLE SCHOOL    98   \n",
       "0  PHELPS ARCHITECTURE CONSTRUCTION AND ENGINEERI...    97   \n",
       "0                           ELIOT HINE MIDDLE SCHOOL    95   \n",
       "0                           POWELL ELEMENTARY SCHOOL    98   \n",
       "\n",
       "                                       original_name  \n",
       "0               RANDL E-HIGH LANDS ELEMENTARY SCHOOL  \n",
       "0                           HEARST ELEMENTARY SCHOOL  \n",
       "0                            EATON ELEMENTARY SCHOOL  \n",
       "0                              KRAME R MIDDLE SCHOOL  \n",
       "0  PHELPS ARCHITECTURE, CONSTRUCTION, AND ENG INE...  \n",
       "0                                 HINE MIDDLE SCHOOL  \n",
       "0                          POWE LL ELEMENTARY SCHOOL  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CHILDRENS GUIL D - BALTIMORE',\n",
       " '07/18/2014',\n",
       " 'FRANCIS-STEVENSEDUCATION CENTER',\n",
       " 'HARDY M S @ HAMILTON SCHOOL',\n",
       " 'BELL M ULTIC ULT URAL SENIOR HIGH SCHOOL',\n",
       " 'WATK INS ELEMENTARY SCHOOL',\n",
       " 'W ILLIAM E. DOAR, JR. PCS FOR THE PERFORMING ARTS',\n",
       " 'SCHOOL-WITHIN-SC HOOL@ PEABODY',\n",
       " 'ADV ANCE PATH',\n",
       " '03/26/ 2019',\n",
       " 'FRANCIS-STEVENS EDUCATIONC',\n",
       " 'NAT IONA L COLLEGIATE PCS',\n",
       " 'TAKOMA EDUCATIONAL']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzymatch_results_df = pd.concat(fuzzymatch_results_list)\n",
    "fuzzymatch_results_df\n",
    "\n",
    "[name for name in subset_names \n",
    " if name not in fuzzymatch_results_df.original_name.tolist()]\n",
    "\n",
    "\n",
    "## test\n",
    "#test_name = \n",
    "#split_space =  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/raj2/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import shogun\n",
    "from shogun import StringCharFeatures, RAWBYTE\n",
    "from shogun import BinaryLabels\n",
    "from shogun import SubsequenceStringKernel\n",
    "from shogun import LibSVM\n",
    "import spacy\n",
    "!python -m spacy download en\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import numpy as np  \n",
    "import pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import string\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "from heapq import nlargest as _nlargest\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## string cleaning\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "## sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "## link processing\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded imports successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded imports successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general-purpose string/text functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function (from stackoverflow)\n",
    "## that\n",
    "## takes in:\n",
    "## @path: pathname\n",
    "## returns:\n",
    "## string\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text\n",
    "\n",
    "def lower_removepunct(string_toclean):\n",
    "    string_lower = string_toclean.lower()\n",
    "    string_lower_char = \"\".join(x for x in string_lower if x not in string.punctuation) \n",
    "    return(string_lower_char)\n",
    "\n",
    "occurrences = lambda s, lst: (i for i,e in enumerate(lst) if s in e)\n",
    "\n",
    "def get_close_matches_indexes(word, possibilities, n=3, cutoff=0.6):\n",
    "    \"\"\"Use SequenceMatcher to return a list of the indexes of the best \n",
    "    \"good enough\" matches. word is a sequence for which close matches \n",
    "    are desired (typically a string).\n",
    "    possibilities is a list of sequences against which to match word\n",
    "    (typically a list of strings).\n",
    "    Optional arg n (default 3) is the maximum number of close matches to\n",
    "    return.  n must be > 0.\n",
    "    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities\n",
    "    that don't score at least that similar to word are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    if not n >  0:\n",
    "        raise ValueError(\"n must be > 0: %r\" % (n,))\n",
    "    if not 0.0 <= cutoff <= 1.0:\n",
    "        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n",
    "    result = []\n",
    "    s = SequenceMatcher()\n",
    "    s.set_seq2(word)\n",
    "    for idx, x in enumerate(possibilities):\n",
    "        s.set_seq1(x)\n",
    "        if s.real_quick_ratio() >= cutoff and s.quick_ratio() >= cutoff and s.ratio() >= cutoff:\n",
    "            result.append((s.ratio(), idx))\n",
    "\n",
    "    # Move the best scorers to head of list\n",
    "    result = _nlargest(n, result)\n",
    "\n",
    "    # Strip scores for the best n matches\n",
    "    return [x for score, x in result]\n",
    "\n",
    "## function for dtm representation\n",
    "def create_nonmasked_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names())\n",
    "    dtm_dense_named_withid = pd.concat([metadata, dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named)\n",
    "\n",
    "def create_sentiment_dataframe(list_ofsentiment_dicts, metadata):\n",
    "    sentiment_df = pd.DataFrame.from_records(list_ofsentiment_dicts).fillna(0)\n",
    "    sentiment_df.columns = ['sentimentsummary_'+ str(i) for i in sentiment_df.columns]\n",
    "    sentiment_withid = pd.concat([metadata, sentiment_df], axis = 1)\n",
    "    return(sentiment_withid)\n",
    "\n",
    "def create_distance_matrix(full_essays, ids):\n",
    "    string_features = StringCharFeatures(full_essays, RAWBYTE)\n",
    "    sk = SubsequenceStringKernel(string_features, string_features, 3, 0.5)\n",
    "    sk_matrix = sk.get_kernel_matrix()\n",
    "    sk_df = pd.DataFrame(sk_matrix)\n",
    "    sk_df.columns = ['id_'+ str(i) for i in ids]\n",
    "    return(sk_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for case processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_links_todocs(url, prefix = None):\n",
    "    \n",
    "    ## call link and open\n",
    "    request = urllib.request.urlopen(url)\n",
    "    opened_page = request.read()\n",
    "    \n",
    "    ## parse page and find all links\n",
    "    parsed_page = BeautifulSoup(opened_page, \"lxml\")\n",
    "    links_page = parsed_page.findAll('a')\n",
    "    \n",
    "    ## iterate over links and return all\n",
    "    all_links = [a['href'] for a in links_page]\n",
    "    \n",
    "    ## if there's a prefix to subset by, add that\n",
    "    if prefix is not None:\n",
    "        prefix_links = [link for link in all_links if link.startswith(prefix)]\n",
    "        \n",
    "        return(prefix_links)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return(all_links)\n",
    "    \n",
    "## for each case, create dataframe\n",
    "def get_metadata_andtext(parsed_page):\n",
    "    \n",
    "    case_metadata = parsed_page.find_all('meta', attrs = {'name': True})\n",
    "    what_to_extract = ['docket', 'decided', 'caption', 'judge', 'summary']\n",
    "    case_metadata_content = [tag['content'] for tag in case_metadata if tag['name'] in what_to_extract]\n",
    "    case_dict = dict(zip(what_to_extract, case_metadata_content))\n",
    "    case_text = parsed_page.find('p').getText()\n",
    "    case_dict['full_text'] = case_text\n",
    "    case_dict_df = pd.DataFrame(case_dict, index = [0])\n",
    "    return(case_dict_df)\n",
    "    \n",
    "def split_caption(data, nameof_captioncol, delimiter = 'v.'):\n",
    "    \n",
    "    caption_split = data[nameof_captioncol].str.split(delimiter, 1).tolist()\n",
    "    plaintiff = [item[0] if isinstance(item, list) else 'Bad split' for item in caption_split]\n",
    "    defendant = [item[1] if isinstance(item, list) and len(item) > 1 else 'Bad split' for item in caption_split]\n",
    "    return(plaintiff, defendant)\n",
    "\n",
    "def extract_clean_defendants(row):\n",
    "    \n",
    "    if row.count_districts_defendant == 0:\n",
    "\n",
    "        def_clean = ' '.join(row.defendant.split())\n",
    "        return def_clean, None, None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        list_form = row.defendant.split(\" \")\n",
    "        board_positions = list(occurrences(\"BOARD\", list_form))\n",
    "        board_start = [i-5 if i-5 >= 0 else 0 for i in board_positions]\n",
    "        all_dist = []\n",
    "        for i in range(0, len(board_positions)):\n",
    "    \n",
    "            dist_init = list_form[board_start[i]:board_positions[i]+1]\n",
    "            dist_clean_1 = \" \".join([i for i in dist_init if i != 'EDUCATION,' and\n",
    "                                                i != \"AND\" and\n",
    "                                                i != \"EDUCATION\" and\n",
    "                                                i != \"\" and \n",
    "                                                i != \",\"] + [\"OF EDUCATION\"])\n",
    "            dist_clean_2 = ' '.join(dist_clean_1.split())\n",
    "            \n",
    "            all_dist.append(dist_clean_2)\n",
    "            \n",
    "        if len(all_dist) == 0:\n",
    "            \n",
    "            return None, None, None\n",
    " \n",
    "        elif len(all_dist) == 1:\n",
    "            \n",
    "            return all_dist[0], None, None\n",
    "            \n",
    "        elif len(all_dist) == 2:\n",
    "            \n",
    "            return all_dist[0], all_dist[1], None\n",
    "        \n",
    "        elif len(all_dist) >= 3:\n",
    "            \n",
    "            return all_dist[0], all_dist[1], all_dist[2]\n",
    "        \n",
    "def extract_clean_plaintiffs(row):\n",
    "    \n",
    "    if row.count_districts_plaintiff == 0:\n",
    "\n",
    "        def_clean = ' '.join(row.plaintiff.split())\n",
    "        return(['parent'])\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        list_form = row.plaintiff.split(\" \")\n",
    "        board_positions = list(occurrences(\"BOARD\", list_form))\n",
    "        board_start = [i-5 if i-5 >= 0 else 0 for i in board_positions]\n",
    "        all_dist = []\n",
    "        for i in range(0, len(board_positions)):\n",
    "    \n",
    "            dist_init = list_form[board_start[i]:board_positions[i]+1]\n",
    "            dist_clean_1 = \" \".join([i for i in dist_init if i != 'EDUCATION,' and\n",
    "                                                i != \"AND\" and\n",
    "                                                i != \"EDUCATION\" and\n",
    "                                                i != \"\" and \n",
    "                                                i != \",\"] + [\"OF EDUCATION\"])\n",
    "            dist_clean_2 = ' '.join(dist_clean_1.split())\n",
    "            \n",
    "            all_dist.append(dist_clean_2)\n",
    "            \n",
    "            return([all_dist[0]])\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step one: read in data from scraping/cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_df = pd.read_pickle('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/all_cases_df_1026.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bad split                                                                                             22\n",
       "  WASHINGTON TOWNSHIPBOARD OF EDUCATION,                                                               5\n",
       "  JERSEY CITY BOARD OF EDUCATION                                                                       4\n",
       "  WILLINGBORO TOWNSHIP BOARD OF  EDUCATION AND BURLINGTON COUNTY  SPECIAL SERVICES SCHOOL DISTRICT     4\n",
       "  EAST ORANGE BOARD OF EDUCATION                                                                       4\n",
       " MIDDLETOWN TOWNSHIP  BOARD OF EDUCATION                                                               4\n",
       "  PAULSBORO BOARD OF EDUCATION ANDJERSEY CITY BOARD OF EDUCATION,                                      3\n",
       "  J.T.,                                                                                                3\n",
       "  MILLBURN TOWNSHIP BOARD OF EDUCATION                                                                 3\n",
       "  South Brunswick Board of Education                                                                   3\n",
       "  FRANKLIN TOWNSHIP BOARD OF EDUCATION                                                                 3\n",
       "  NEWARK BOARD OF EDUCATION                                                                            3\n",
       "  NEWARK BOARD OF EDUCATION,                                                                           3\n",
       "    ROCKAWAY TOWNSHIP BOARD OF EDUCATION                                                               3\n",
       "  Deptford Township Board of Education                                                                 3\n",
       "  PRINCETON REGIONAL   BOARD OF EDUCATION                                                              3\n",
       "  WEST ORANGE BOARD OF EDUCATION                                                                       3\n",
       "  R.M. AND V.M. ON BEHALF OF J.M.                                                                      2\n",
       "  FRANKLIN TOWNSHIPBOARD OF EDUCATION,                                                                 2\n",
       "  EDISON TOWNSHIP BOARD OF EDUCATION                                                                   2\n",
       "  WILLINGBORO TOWNSHIPBOARD OF EDUCATION,                                                              2\n",
       "  PASSAIC CITY BOARD OF EDUCATION                                                                      2\n",
       "  Haddon Township Board of Education                                                                   2\n",
       "  Kenilworth Board of Education                                                                        2\n",
       "  Union Township Board of Education                                                                    2\n",
       "  WASHINGTON TOWNSHIP BOARD OF EDUCATION                                                               2\n",
       "  SOUTH BRUNSWICK BOARDOF EDUCATION,                                                                   2\n",
       "  West Windsor-Plainsboro Regional Board of Ed.                                                        2\n",
       "  Pemberton Township Board of Education                                                                2\n",
       "  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION                                                           2\n",
       "                                                                                                      ..\n",
       "  Lenape Regional Board of Education                                                                   1\n",
       "  S.R. ON BEHALF OF M.R.                                                                               1\n",
       " BRICK TOWNSHIP BOARD  OF EDUCATION                                                                    1\n",
       "  J.R. and J.R. on behalf of E.R.                                                                      1\n",
       "    SOMERSET HILLS REGIONAL SCHOOL DISTRICT                                                            1\n",
       "  READINGTON TOWNSHIPBOARD OF EDUCATION,                                                               1\n",
       "  Manasquan Board of Education                                                                         1\n",
       "  Wayne Township Board of Educaton                                                                     1\n",
       "  Freehold Regional High School                                                                        1\n",
       "  TOMS RIVERBOARD OF EDUCATION,                                                                        1\n",
       "  R.O.,                                                                                                1\n",
       "  Hunterdon Central Regional High School Board of Education                                            1\n",
       "  CRANBURY TOWNSHIP BOARD OF EDUCATION                                                                 1\n",
       "L.C. AND K.C. ON BEHALF OF A.C.                                                                        1\n",
       " MONTGOMERY TOWNSHIP BOARD OF EDUCATION,                                                               1\n",
       "  D.C.,                                                                                                1\n",
       "  Medford Township Board of Education                                                                  1\n",
       "    MOORESTOWN TOWNSHIP   BOARD OF EDUCATION                                                           1\n",
       "  SALEM CITY BOARD OF EDUCATION                                                                        1\n",
       "  R.R., O/B/O D.L.,                                                                                    1\n",
       " AUDUBON BOARD OF EDUCATION                                                                            1\n",
       "  BRIDGEWATER-RARITAN REGIONAL BOARD OF EDUCATION                                                      1\n",
       "  PENNSAUKEN BOARD OF EDUCATION,                                                                       1\n",
       "  MANVILLE BOARD OF EDUCATION,                                                                         1\n",
       "  Manalapan-Englishtown Regional Board of                                                              1\n",
       "  J.B.,                                                                                                1\n",
       "  RIVERSIDE BOARD OF EDUCATION AND NEW JERSEY STATE INTERSCHOLASTIC ATHLETIC ASSOCIATION,              1\n",
       " PEMBERTON TOWNSHIP BOARDOF EDUCATION,                                                                 1\n",
       "  Collingswood Board of Education                                                                      1\n",
       "  MERCHANTVILLE BOARD OF EDUCATION                                                                     1\n",
       "Name: defendant, Length: 826, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cases_df.defendant.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step two: cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: clean bad caption splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "Name: plaintiff, dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "Name: defendant, dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "Name: why_bad_split, dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for non-NA caption\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "all_cases_df['why_bad_split'] = np.where(all_cases_df.caption.isnull(), 'no_caption', \n",
    "                                np.where((~all_cases_df.caption.isnull()) & \n",
    "                                        (all_cases_df.defendant == 'Bad split'), 'caption_badsplit',\n",
    "                                        'caption'))\n",
    "\n",
    "## for ones that have a caption and bad split, manually code\n",
    "all_cases_df_goodsplit = all_cases_df.loc[all_cases_df.why_bad_split == 'caption'].copy()\n",
    "all_cases_df_havecaption_badsplit = all_cases_df.loc[all_cases_df.why_bad_split == 'caption_badsplit'].copy()\n",
    "all_cases_df_havecaption_badsplit.defendant[all_cases_df_havecaption_badsplit.docket == 'eds02189-02_1']  = 'TEWKSBURY TOWNSHIP BOARD OF EDUCATION'\n",
    "all_cases_df_havecaption_badsplit.plaintiff[all_cases_df_havecaption_badsplit.docket == 'eds02189-02_1']  == 'G.R. O/B/O L.R.'\n",
    "all_cases_df_havecaption_badsplit.plaintiff[all_cases_df_havecaption_badsplit.docket == 'eds03684-04_1']  = 'UNION TOWNSHIP BOARD OF EDUCATION'\n",
    "all_cases_df_havecaption_badsplit.defendant[all_cases_df_havecaption_badsplit.docket == 'eds03684-04_1']  == 'L. J. O/B/O N. M.'\n",
    "all_cases_df_havecaption_badsplit.why_bad_split[all_cases_df_havecaption_badsplit.docket == 'eds05088-05_1']  == 'no_caption'\n",
    "all_cases_df_havecaption_badsplit.defendant[all_cases_df_havecaption_badsplit.docket == 'eds08699-03_1']  = 'NORTH PLAINFIELD BOARD OF EDUCATION'\n",
    "all_cases_df_havecaption_badsplit.plaintiff[all_cases_df_havecaption_badsplit.docket == 'eds08699-03_1']  = 'Unknown O/B/O'\n",
    "all_cases_df_havecaption_badsplit.why_bad_split[all_cases_df_havecaption_badsplit.docket == 'eds1528-01_1']  = 'not_IDEA_case'\n",
    "all_cases_df_havecaption_badsplit.plaintiff[all_cases_df_havecaption_badsplit.docket == 'eds5858-97']  = 'STILLWATER BOARD OF EDUCATION'\n",
    "all_cases_df_havecaption_badsplit.defendant[all_cases_df_havecaption_badsplit.docket == 'eds5858-97']  = 'Unknown O/B/O'\n",
    "all_cases_df_havecaption_badsplit.plaintiff[all_cases_df_havecaption_badsplit.docket == 'eds5858-97']  = 'STILLWATER BOARD OF EDUCATION'\n",
    "all_cases_df_havecaption_badsplit.plaintiff[all_cases_df_havecaption_badsplit.docket == 'eds10747-14_1']  = 'R.S. AND K.S. ON BEHALF OF J.S.'\n",
    "all_cases_df_havecaption_badsplit.defendant[all_cases_df_havecaption_badsplit.docket == 'eds10747-14_1']  = 'MANALAPAN-ENGLISHTOWN REGIONAL BOARD OF EDUCATION'\n",
    "\n",
    "## re-merge\n",
    "all_cases_df_havecaption_badsplit_tobind = all_cases_df_havecaption_badsplit.loc[~all_cases_df_havecaption_badsplit.why_bad_split.isin(['no_caption',\n",
    "                                                                                                        'not_IDEA_case'])]\n",
    "\n",
    "## set of cases\n",
    "all_cases_df_analytic = pd.concat([all_cases_df_goodsplit, \n",
    "                                  all_cases_df_havecaption_badsplit_tobind],\n",
    "                                 axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 clean multiple defendants \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make everything all caps and then check if has multiple board of education\n",
    "all_cases_df_analytic.plaintiff = all_cases_df_analytic.plaintiff.str.upper()\n",
    "all_cases_df_analytic.defendant = all_cases_df_analytic.defendant.str.upper()\n",
    "all_cases_df_analytic['count_districts_plaintiff'] = all_cases_df_analytic.plaintiff.str.count('BOARD(\\\\s)?(OF)?(\\\\s)?EDUCATION')\n",
    "all_cases_df_analytic['count_districts_defendant'] = all_cases_df_analytic.defendant.str.count('BOARD(\\\\s)?(OF)?(\\\\s)?EDUCATION')\n",
    "\n",
    "cleaned_defendants = all_cases_df_analytic.apply(extract_clean_defendants, \n",
    "                                                                    axis = 1)\n",
    "cleaned_plaintiffs = all_cases_df_analytic.apply(extract_clean_plaintiffs, \n",
    "                                                                    axis = 1)\n",
    "cleaned_defendants_list = cleaned_defendants.tolist()\n",
    "cleaned_plaintiffs_list = cleaned_plaintiffs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_df_analytic['clean_defendant_1'] = [i[0] for i in cleaned_defendants_list]\n",
    "all_cases_df_analytic['clean_defendant_2'] = [i[1] for i in cleaned_defendants_list]\n",
    "all_cases_df_analytic['clean_defendant_3'] = [i[2] for i in cleaned_defendants_list]\n",
    "all_cases_df_analytic['clean_plaintiff_1'] = [i[0] for i in cleaned_plaintiffs_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: merge defendants and case captions with complaint metadata based on caption and case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nces_name</th>\n",
       "      <th>doe_name</th>\n",
       "      <th>AgencyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCH DIST OF THE CHATHAMS</td>\n",
       "      <td>SCH DIST OF THE CHATHAMS</td>\n",
       "      <td>THE SCHOOL DISTRICT OF THE CHATHAMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GREAT MEADOWS REGIONAL</td>\n",
       "      <td>GREAT MEADOWS REGIONAL</td>\n",
       "      <td>GREAT MEADOWS REGIONAL SCHOOL DISTRICT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SOMERSET HILLS REGIONAL</td>\n",
       "      <td>SOMERSET HILLS REGIONAL</td>\n",
       "      <td>SOMERSET HILLS REGIONAL SCHOOL DISTRICT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OCEANSIDE CHARTER SCHOOL</td>\n",
       "      <td>OCEANSIDE CS</td>\n",
       "      <td>OCEANSIDE CHARTER SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PLEASANTECH ACADEMY CHART</td>\n",
       "      <td>PLEASANTECH ACADEMY CS</td>\n",
       "      <td>PLEASANTECH ACADEMY CHARTER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   nces_name                  doe_name  \\\n",
       "0  SCH DIST OF THE CHATHAMS   SCH DIST OF THE CHATHAMS   \n",
       "1  GREAT MEADOWS REGIONAL     GREAT MEADOWS REGIONAL     \n",
       "2  SOMERSET HILLS REGIONAL    SOMERSET HILLS REGIONAL    \n",
       "3  OCEANSIDE CHARTER SCHOOL   OCEANSIDE CS               \n",
       "4  PLEASANTECH ACADEMY CHART  PLEASANTECH ACADEMY CS     \n",
       "\n",
       "                                AgencyName  \n",
       "0  THE SCHOOL DISTRICT OF THE CHATHAMS      \n",
       "1  GREAT MEADOWS REGIONAL SCHOOL DISTRICT   \n",
       "2  SOMERSET HILLS REGIONAL SCHOOL DISTRICT  \n",
       "3  OCEANSIDE CHARTER SCHOOL                 \n",
       "4  PLEASANTECH ACADEMY CHARTER              "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## what to load: \n",
    "## 1. raw captions + \n",
    "## 2. cleaned data\n",
    "case_metadata_raw = pd.read_csv('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/cleaned/cleaned_casecaptions_20181211.csv')\n",
    "district_dem_raw =  pd.read_csv('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/cleaned/dist_analytic_4spatialproj.csv')\n",
    "district_dem_raw[['nces_name', 'doe_name', 'AgencyName']].head()\n",
    "\n",
    "## look at intersection with case metadata\n",
    "case_metadata_names = set(case_metadata_raw.match_in_df.unique())\n",
    "nces_names = set(district_dem_raw.nces_name.unique())\n",
    "district_universe = list(set(case_metadata_names).union(nces_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_metadata_raw.columns = ['metadata_docket_noprefix', 'metadata_caption', 'metadata_status', 'metadata_decision', \n",
    "                             'metadata_springschoolyear_opened',\n",
    "                            'metadata_springschoolyear_closed', \n",
    "                            'metadata_district_name', \n",
    "                            'metadata_LEAID', \n",
    "                            'metadata_parentdefendant']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_df_analytic_tomatch = all_cases_df_analytic[['caption', 'summary', 'docket',\n",
    "                                                       'count_districts_defendant',\n",
    "                                                      'clean_defendant_1', \n",
    "                                                      'clean_defendant_2',\n",
    "                                                      'clean_defendant_3',\n",
    "                                                      'clean_plaintiff_1']].copy()\n",
    "\n",
    "## 931 to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different approach: treat metadata district names as universe of districts and do partial match\n",
    "### use different universe: union of case_metadata districts and districts in analytic sample\n",
    "\n",
    "## for remaining non-parent defendants, find closest match\n",
    "dist_pattern_toremove = \"BOARD(OF)?(\\\\s+)?(OF)?(\\\\s+)?EDUCATION(\\\\s+)?|SCHOOL(\\\\s+)?DISTRICT(\\\\s+)?|\\\\s+$\"\n",
    "all_cases_df_analytic_tomatch['clean_defendant_1_formatch'] = all_cases_df_analytic_tomatch.clean_defendant_1.str.replace(dist_pattern_toremove, \"\")\n",
    "all_cases_df_analytic_tomatch['clean_defendant_2_formatch'] = all_cases_df_analytic_tomatch.clean_defendant_2.str.replace(dist_pattern_toremove, \"\") \n",
    "all_cases_df_analytic_tomatch['clean_defendant_3_formatch'] = all_cases_df_analytic_tomatch.clean_defendant_3.str.replace(dist_pattern_toremove, \"\")\n",
    "all_cases_df_analytic_tomatch['clean_plaintiff_formatch'] = all_cases_df_analytic_tomatch.clean_plaintiff_1.str.replace(dist_pattern_toremove, \"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_fromcases_clean = all_cases_df_analytic_tomatch.clean_defendant_1_formatch\n",
    "districts_fromcases_clean_def2 = all_cases_df_analytic_tomatch.clean_defendant_2_formatch\n",
    "districts_fromcases_clean_def3 = all_cases_df_analytic_tomatch.clean_defendant_3_formatch\n",
    "districts_fromcases_clean_plaint = all_cases_df_analytic_tomatch.clean_plaintiff_formatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    }
   ],
   "source": [
    "## find closest matches\n",
    "district_match = [process.extractBests(x, district_universe, \n",
    "                scorer = fuzz.partial_token_sort_ratio, score_cutoff = 30) for x in districts_fromcases_clean]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_match_def2 = [process.extractBests(x, district_universe, \n",
    "                scorer = fuzz.partial_token_sort_ratio, score_cutoff = 30)\n",
    "                if x is not None \n",
    "                else [('None', 'None')]\n",
    "                for x in districts_fromcases_clean_def2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_match_def3 = [process.extractBests(x, district_universe, \n",
    "                scorer = fuzz.partial_token_sort_ratio, score_cutoff = 30) \n",
    "                if x is not None \n",
    "                else [('None', 'None')]\n",
    "                for x in districts_fromcases_clean_def3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    }
   ],
   "source": [
    "district_match_plaintiff = [process.extractBests(x, district_universe, \n",
    "                scorer = fuzz.partial_token_sort_ratio, score_cutoff = 30) \n",
    "                if x is not \"parent\" \n",
    "                else [('parent', 'parent')] \n",
    "                for x in districts_fromcases_clean_plaint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_match = [item[0][0] if len(item) > 0 else None for item in district_match]\n",
    "second_match = [item[1][0] if len(item) > 0 else None for item in district_match]\n",
    "third_match = [item[2][0] if len(item) > 0 else None for item in district_match]\n",
    "first_match_def2 = [item[0][0] if len(item) > 0 else None for item in district_match_def2]\n",
    "first_match_def3 = [item[0][0] if len(item) > 0 else None for item in district_match_def3]\n",
    "first_match_plaintiff = [item[0][0] if len(item) > 0 else None for item in district_match_plaintiff]\n",
    "\n",
    "\n",
    "## bind into a dataframe\n",
    "districts_withmatches = pd.DataFrame({'original_district' : districts_fromcases_clean,\n",
    " 'first_match' : first_match,\n",
    " 'second_match': second_match,\n",
    "'third_match': third_match,\n",
    "'first_match_def2': first_match_def2,\n",
    "'first_match_def3': first_match_def3,\n",
    "'first_match_plaintiff': first_match_plaintiff,                           \n",
    "  })\n",
    "\n",
    "\n",
    "## sort by original_district and write to csv\n",
    "districts_withmatches_tomerge = districts_withmatches.drop_duplicates()\n",
    "\n",
    "\n",
    "## write to intermediate file to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(965, 14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## merge with original\n",
    "districts_withmatches_tocheck_moreinfo = pd.merge(districts_withmatches_tomerge[['original_district', \n",
    "                                                                                'first_match',\n",
    "                                                                                'second_match',\n",
    "                                                                                'third_match',\n",
    "                                                                                'first_match_def2',\n",
    "                                                                                'first_match_def3',\n",
    "                                                                                'first_match_plaintiff']],\n",
    "                                                 all_cases_df_analytic_tomatch[['caption',\n",
    "                                                                                'docket',\n",
    "                                                                    'clean_defendant_1_formatch',\n",
    "                                                                    'clean_defendant_1',\n",
    "                                                                    'clean_defendant_2_formatch',\n",
    "                                                                    'clean_defendant_3_formatch',\n",
    "                                                                    'clean_plaintiff_formatch']],\n",
    "                                                 left_on = 'original_district',\n",
    "                                                 right_on = 'clean_defendant_1_formatch',\n",
    "                                                 how = 'inner')\n",
    "\n",
    "\n",
    "districts_withmatches_tocheck_moreinfo.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1: clean the matches for the primary district defendant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## #districts_withmatches_tocheck_moreinfo.to_csv('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/hearingmatches_tocheck.csv')\n",
    "\n",
    "\n",
    "## read in\n",
    "cleaned_matches = pd.read_csv('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/hearingmatches_tocheck_rjedits20181212.csv',\n",
    "                            encoding='latin1')\n",
    "cleaned_matches.shape\n",
    "cleaned_matches['final_defendant_1'] = np.where(cleaned_matches.best_match == 1,\n",
    "                                            cleaned_matches.first_match,\n",
    "                                    np.where(cleaned_matches.best_match == 2, \n",
    "                                            cleaned_matches.second_match,\n",
    "                                            cleaned_matches.third_match))\n",
    "\n",
    "cleaned_matches_tomerge = cleaned_matches[['docket', 'caption', 'final_defendant_1']].copy()\n",
    "\n",
    "## next steps:\n",
    "## 1. code to best match (np.where etc)\n",
    "## 2. merge with case text on the basis of docket number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2: clean the matches for the other districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## next steps\n",
    "## 1. subset to ones with non-empty extra defendants and non parent plaintiffs\n",
    "districts_withmatches_tocheck_additional = districts_withmatches_tocheck_moreinfo.loc[(districts_withmatches_tocheck_moreinfo.first_match_def2 != 'None') |\n",
    "                                                        (districts_withmatches_tocheck_moreinfo.first_match_def3 != 'None') |\n",
    "                                                    (districts_withmatches_tocheck_moreinfo.clean_plaintiff_formatch != 'parent'),\n",
    "                                                    ['caption', 'clean_defendant_2_formatch',\n",
    "                                                     'first_match_def2',\n",
    "                                                        'clean_defendant_3_formatch',\n",
    "                                                    'first_match_def3',\n",
    "                                                        'clean_plaintiff_formatch',\n",
    "                                                    'first_match_plaintiff',\n",
    "                                                    'docket']]\n",
    "districts_withmatches_tocheck_additional['final_defendant_2'] = districts_withmatches_tocheck_additional['first_match_def2']\n",
    "districts_withmatches_tocheck_additional['final_defendant_3'] = districts_withmatches_tocheck_additional['first_match_def3']\n",
    "districts_withmatches_tocheck_additional['final_plaintiff'] = districts_withmatches_tocheck_additional['first_match_plaintiff']\n",
    "\n",
    "#districts_withmatches_tocheck_additional.to_csv('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/additionalmatches_tocheck_rjedits20181212.csv',\n",
    " #                                              index = False)\n",
    "\n",
    "## read in manually checked districts\n",
    "cleaned_matches_more = pd.read_csv('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/additionalmatches_tocheck_rjedits20181212.csv',\n",
    "                            encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docket</th>\n",
       "      <th>caption</th>\n",
       "      <th>final_defendant_2</th>\n",
       "      <th>final_defendant_3</th>\n",
       "      <th>final_plaintiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eds00021-05_1</td>\n",
       "      <td>B.G. AND M.J. O/B/O D.G .,    v.    SOMERSET HILLS REGIONAL BOARD OF EDUCATION AND BEDMINSTER TOWNSHIP BOARD OF EDUCATION</td>\n",
       "      <td>BEDMINSTER TOWNSHIP</td>\n",
       "      <td>None</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eds00727-04_1</td>\n",
       "      <td>R.K. O/B/O S.K.,    v.    SOMERSET HILLS REGIONAL SCHOOL DISTRICT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eds00250-04_1</td>\n",
       "      <td>WASHINGTON TOWNSHIP BOARD OF EDUCATION,  v. H.B. ON BEHALF OF H.B.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WASHINGTON TOWNSHIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eds02036-05_1</td>\n",
       "      <td>WASHINGTON TOWNSHIP BOARD OF EDUCATION,    v.  H.B. ON BEHALF OF H.B.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WASHINGTON TOWNSHIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eds00592-04_1</td>\n",
       "      <td>OCEAN TOWNSHIP BOARD OF EDUCATION,  v.  J. E. AND T.B. ON BEHALF OF J. E.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>OCEAN TOWNSHIP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          docket  \\\n",
       "0  eds00021-05_1   \n",
       "1  eds00727-04_1   \n",
       "2  eds00250-04_1   \n",
       "3  eds02036-05_1   \n",
       "4  eds00592-04_1   \n",
       "\n",
       "                                                                                                                     caption  \\\n",
       "0  B.G. AND M.J. O/B/O D.G .,    v.    SOMERSET HILLS REGIONAL BOARD OF EDUCATION AND BEDMINSTER TOWNSHIP BOARD OF EDUCATION   \n",
       "1  R.K. O/B/O S.K.,    v.    SOMERSET HILLS REGIONAL SCHOOL DISTRICT                                                           \n",
       "2  WASHINGTON TOWNSHIP BOARD OF EDUCATION,  v. H.B. ON BEHALF OF H.B.                                                          \n",
       "3  WASHINGTON TOWNSHIP BOARD OF EDUCATION,    v.  H.B. ON BEHALF OF H.B.                                                       \n",
       "4  OCEAN TOWNSHIP BOARD OF EDUCATION,  v.  J. E. AND T.B. ON BEHALF OF J. E.                                                   \n",
       "\n",
       "     final_defendant_2 final_defendant_3      final_plaintiff  \n",
       "0  BEDMINSTER TOWNSHIP  None              parent               \n",
       "1  None                 None              parent               \n",
       "2  None                 None              WASHINGTON TOWNSHIP  \n",
       "3  None                 None              WASHINGTON TOWNSHIP  \n",
       "4  None                 None              OCEAN TOWNSHIP       "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_matches_more_tomerge = cleaned_matches_more[['docket', 'caption', \n",
    "                                    'final_defendant_2','final_defendant_3',\n",
    "                                    'final_plaintiff']].drop_duplicates()\n",
    "\n",
    "cleaned_matches_more_tomerge.head()\n",
    "\n",
    "## next steps: subset to final defendant_2, 3, and plaintiff\n",
    "## and then merge with the other cleaned ones after coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3: merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(607, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(28, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>docket</th>\n",
       "      <th>final_defendant_1</th>\n",
       "      <th>final_defendant_2</th>\n",
       "      <th>final_defendant_3</th>\n",
       "      <th>final_plaintiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON TOWNSHIP BOARD OF EDUCATION,  v. H.B. ON BEHALF OF H.B.</td>\n",
       "      <td>eds00250-04_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WASHINGTON TOWNSHIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON TOWNSHIP BOARD OF EDUCATION,    v.  H.B. ON BEHALF OF H.B.</td>\n",
       "      <td>eds02036-05_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WASHINGTON TOWNSHIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OCEAN TOWNSHIP BOARD OF EDUCATION,  v.  J. E. AND T.B. ON BEHALF OF J. E.</td>\n",
       "      <td>eds00592-04_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>OCEAN TOWNSHIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GLOUCESTER CITY BOARD OF EDUCATION  v.  D.H. O/B/O J.H.</td>\n",
       "      <td>eds00699-03_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>GLOUCESTER CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GLOUCESTER CITY BOARD OF EDUCATION  v.  D.H. and T.H. o/b/o D.H.</td>\n",
       "      <td>eds00724-03_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>GLOUCESTER CITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     caption  \\\n",
       "2  WASHINGTON TOWNSHIP BOARD OF EDUCATION,  v. H.B. ON BEHALF OF H.B.          \n",
       "3  WASHINGTON TOWNSHIP BOARD OF EDUCATION,    v.  H.B. ON BEHALF OF H.B.       \n",
       "4  OCEAN TOWNSHIP BOARD OF EDUCATION,  v.  J. E. AND T.B. ON BEHALF OF J. E.   \n",
       "5  GLOUCESTER CITY BOARD OF EDUCATION  v.  D.H. O/B/O J.H.                     \n",
       "6  GLOUCESTER CITY BOARD OF EDUCATION  v.  D.H. and T.H. o/b/o D.H.            \n",
       "\n",
       "          docket final_defendant_1 final_defendant_2 final_defendant_3  \\\n",
       "2  eds00250-04_1  parent            None              None               \n",
       "3  eds02036-05_1  parent            None              None               \n",
       "4  eds00592-04_1  parent            None              None               \n",
       "5  eds00699-03_1  parent            None              None               \n",
       "6  eds00724-03_1  parent            None              None               \n",
       "\n",
       "       final_plaintiff  \n",
       "2  WASHINGTON TOWNSHIP  \n",
       "3  WASHINGTON TOWNSHIP  \n",
       "4  OCEAN TOWNSHIP       \n",
       "5  GLOUCESTER CITY      \n",
       "6  GLOUCESTER CITY      "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docket_onedefendant = cleaned_matches_tomerge.docket.tolist()\n",
    "docket_multipledefendants = cleaned_matches_more_tomerge.docket.tolist()\n",
    "docket_inboth = list(set(docket_onedefendant).intersection(set(docket_multipledefendants)))\n",
    "\n",
    "## ones with district as plaintiff\n",
    "cleaned_matches_more_notinboth = cleaned_matches_more_tomerge.loc[(~cleaned_matches_more_tomerge.docket.isin(docket_inboth)) &\n",
    "                                                                    (cleaned_matches_more_tomerge.docket != 'eds00727-04_1')].copy()\n",
    "cleaned_matches_more_notinboth['final_defendant_1'] = 'parent'\n",
    "\n",
    "## ones with district as defendant but multiple districts\n",
    "cleaned_matches_more_inboth = cleaned_matches_more_tomerge.loc[(cleaned_matches_more_tomerge.docket.isin(docket_inboth)) |\n",
    "                            (cleaned_matches_more_tomerge.docket == 'eds00727-04_1')].copy()\n",
    "\n",
    "cleaned_matches_tomerge_inboth = cleaned_matches_tomerge[cleaned_matches_tomerge.docket.isin(docket_inboth)].copy()\n",
    "\n",
    "## merge the ones that are in both\n",
    "cleaned_matches_districtdefendants_torbind = pd.merge(cleaned_matches_more_inboth,\n",
    "                                              cleaned_matches_tomerge_inboth[['docket', 'final_defendant_1']],\n",
    "                                              on = 'docket')\n",
    "\n",
    "## for the ones that are not in both, add extra columns\n",
    "cleaned_matches_tomerge_notinboth = cleaned_matches_tomerge[~cleaned_matches_tomerge.docket.isin(docket_inboth)].copy()\n",
    "cleaned_matches_tomerge_notinboth['final_defendant_2'] = 'None'\n",
    "cleaned_matches_tomerge_notinboth['final_defendant_3'] = 'None'\n",
    "cleaned_matches_tomerge_notinboth['final_plaintiff'] = 'parent'\n",
    "cleaned_matches_more_notinboth.shape\n",
    "cleaned_matches_tomerge_notinboth.shape\n",
    "cleaned_matches_districtdefendants_torbind.shape\n",
    "cleaned_matches_init = pd.concat([cleaned_matches_more_notinboth,\n",
    "                               cleaned_matches_tomerge_notinboth,\n",
    "                               cleaned_matches_districtdefendants_torbind]).drop_duplicates()\n",
    "cleaned_matches_init.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see which ids are missing from final dataset\n",
    "missing_docket = set(all_cases_df_analytic.docket.tolist()).difference(cleaned_matches_init.docket.tolist())\n",
    "missing_df = all_cases_df_analytic[all_cases_df_analytic.docket.isin(missing_docket)].copy()\n",
    "\n",
    "## deal with those and the 100-ish still missing\n",
    "## match \n",
    "missing_df['count_districts_plaintiff'] = np.where(missing_df.plaintiff.str.contains(\"BOARD|TOWNSHIP|DISTRICT\"), 1, 0)\n",
    "cleaned_plaintiffs = missing_df.apply(extract_clean_plaintiffs, axis = 1)\n",
    "cleaned_plaintiffs_list = cleaned_plaintiffs.tolist()\n",
    "cleaned_plaintiffs_list_updated = [['parent'] if x is None else x for x in cleaned_plaintiffs_list]\n",
    "missing_df['clean_plaintiff_1'] = [i[0] for i in cleaned_plaintiffs_list_updated]\n",
    "#missing_df[['caption', 'plaintiff', 'defendant', 'clean_defendant_1', \n",
    " #           'clean_plaintiff_1', 'count_districts_plaintiff']]\n",
    "\n",
    "## fuzzy matching\n",
    "missing_df['clean_defendant_1_formatch'] = missing_df.clean_defendant_1.str.replace(dist_pattern_toremove, \"\")\n",
    "missing_df['clean_plaintiff_formatch'] = missing_df.clean_plaintiff_1.str.replace(dist_pattern_toremove, \"\")\n",
    "\n",
    "missing_district_match = [process.extractBests(x, district_universe, \n",
    "                scorer = fuzz.partial_token_sort_ratio, score_cutoff = 30) for x in missing_df.clean_defendant_1_formatch.tolist()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_plaintiff_match = [process.extractBests(x, district_universe, \n",
    "                scorer = fuzz.partial_token_sort_ratio, score_cutoff = 30) \n",
    "                if x is not \"parent\" \n",
    "                else [('parent', 'parent')] \n",
    "                for x in missing_df.clean_plaintiff_formatch]\n",
    "\n",
    "first_match = [item[0][0] if len(item) > 0 else None for item in missing_district_match]\n",
    "first_plaintiff_match = [item[0][0] if len(item) > 0 else None for item in missing_plaintiff_match]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>docket</th>\n",
       "      <th>final_plaintiff</th>\n",
       "      <th>final_defendant_1</th>\n",
       "      <th>final_defendant_2</th>\n",
       "      <th>final_defendant_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D.C. AND J.P. O/B/O K.C.,    v.  LAWRENCE TOWNSHIP BOARD   OF EDUCATION</td>\n",
       "      <td>eds00050-04_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>LAWRENCE TOWNSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D.B. AND M.B. O/B/O C.B.,    v.  BERNARDS TOWNSHIP BOARD   OF EDUCATION</td>\n",
       "      <td>eds00412-06_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>BERNARDS TOWNSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L.U. O/B/O A.P.,    v.  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION</td>\n",
       "      <td>eds00566-05_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>PEMBERTON TOWNSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L.U. O/B/O A.P.,    v.  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION</td>\n",
       "      <td>eds00566-05_3</td>\n",
       "      <td>parent</td>\n",
       "      <td>PEMBERTON TOWNSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R.K. O/B/O S.K.,    v.    SOMERSET HILLS REGIONAL SCHOOL DISTRICT</td>\n",
       "      <td>eds00727-04_1</td>\n",
       "      <td>parent</td>\n",
       "      <td>SOMERSET HILLS REGIONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   caption  \\\n",
       "0  D.C. AND J.P. O/B/O K.C.,    v.  LAWRENCE TOWNSHIP BOARD   OF EDUCATION   \n",
       "1  D.B. AND M.B. O/B/O C.B.,    v.  BERNARDS TOWNSHIP BOARD   OF EDUCATION   \n",
       "2  L.U. O/B/O A.P.,    v.  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION        \n",
       "3  L.U. O/B/O A.P.,    v.  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION        \n",
       "4  R.K. O/B/O S.K.,    v.    SOMERSET HILLS REGIONAL SCHOOL DISTRICT         \n",
       "\n",
       "          docket final_plaintiff        final_defendant_1 final_defendant_2  \\\n",
       "0  eds00050-04_1  parent          LAWRENCE TOWNSHIP        NaN                \n",
       "1  eds00412-06_1  parent          BERNARDS TOWNSHIP        NaN                \n",
       "2  eds00566-05_1  parent          PEMBERTON TOWNSHIP       NaN                \n",
       "3  eds00566-05_3  parent          PEMBERTON TOWNSHIP       NaN                \n",
       "4  eds00727-04_1  parent          SOMERSET HILLS REGIONAL  NaN                \n",
       "\n",
       "  final_defendant_3  \n",
       "0  NaN               \n",
       "1  NaN               \n",
       "2  NaN               \n",
       "3  NaN               \n",
       "4  NaN               "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df['preliminary_plaintiff'] = first_plaintiff_match\n",
    "missing_df['preliminary_defendant'] = first_match\n",
    "missing_df['preliminary_defendant'] = np.where(missing_df.preliminary_plaintiff != \"parent\",\n",
    "                                              \"parent\", missing_df.preliminary_defendant)\n",
    "missing_df_tomerge = missing_df[['caption', 'docket', 'preliminary_plaintiff', 'preliminary_defendant']].copy()\n",
    "#missing_df_tomerge.to_csv('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/missingmatches_tocheck.csv',\n",
    " #                        index = False)\n",
    "    \n",
    "missing_df_clean = pd.read_csv('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/missingmatches_tocheck_rjedits20181212.csv')\n",
    "missing_df_clean.columns = ['caption', 'docket', 'final_plaintiff', \n",
    "                           'final_defendant_1', 'final_defendant_2', 'final_defendant_3']\n",
    "missing_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>docket</th>\n",
       "      <th>final_defendant_1</th>\n",
       "      <th>final_defendant_2</th>\n",
       "      <th>final_defendant_3</th>\n",
       "      <th>final_plaintiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D.C. AND J.P. O/B/O K.C.,    v.  LAWRENCE TOWNSHIP BOARD   OF EDUCATION</td>\n",
       "      <td>eds00050-04_1</td>\n",
       "      <td>LAWRENCE TOWNSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D.B. AND M.B. O/B/O C.B.,    v.  BERNARDS TOWNSHIP BOARD   OF EDUCATION</td>\n",
       "      <td>eds00412-06_1</td>\n",
       "      <td>BERNARDS TOWNSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L.U. O/B/O A.P.,    v.  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION</td>\n",
       "      <td>eds00566-05_1</td>\n",
       "      <td>PEMBERTON TOWNSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L.U. O/B/O A.P.,    v.  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION</td>\n",
       "      <td>eds00566-05_3</td>\n",
       "      <td>PEMBERTON TOWNSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R.K. O/B/O S.K.,    v.    SOMERSET HILLS REGIONAL SCHOOL DISTRICT</td>\n",
       "      <td>eds00727-04_1</td>\n",
       "      <td>SOMERSET HILLS REGIONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   caption  \\\n",
       "0  D.C. AND J.P. O/B/O K.C.,    v.  LAWRENCE TOWNSHIP BOARD   OF EDUCATION   \n",
       "1  D.B. AND M.B. O/B/O C.B.,    v.  BERNARDS TOWNSHIP BOARD   OF EDUCATION   \n",
       "2  L.U. O/B/O A.P.,    v.  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION        \n",
       "3  L.U. O/B/O A.P.,    v.  TOWNSHIP OF PEMBERTON BOARD   OF EDUCATION        \n",
       "4  R.K. O/B/O S.K.,    v.    SOMERSET HILLS REGIONAL SCHOOL DISTRICT         \n",
       "\n",
       "          docket        final_defendant_1 final_defendant_2 final_defendant_3  \\\n",
       "0  eds00050-04_1  LAWRENCE TOWNSHIP        NaN               NaN                \n",
       "1  eds00412-06_1  BERNARDS TOWNSHIP        NaN               NaN                \n",
       "2  eds00566-05_1  PEMBERTON TOWNSHIP       NaN               NaN                \n",
       "3  eds00566-05_3  PEMBERTON TOWNSHIP       NaN               NaN                \n",
       "4  eds00727-04_1  SOMERSET HILLS REGIONAL  NaN               NaN                \n",
       "\n",
       "  final_plaintiff  \n",
       "0  parent          \n",
       "1  parent          \n",
       "2  parent          \n",
       "3  parent          \n",
       "4  parent          "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## merge with the remainder\n",
    "all_cleaned_captions = pd.concat([missing_df_clean, cleaned_matches_init])\n",
    "\n",
    "## now write to pickle\n",
    "all_cleaned_captions.head()\n",
    "\n",
    "otherinfo_tomerge = all_cases_df_analytic[['full_text', 'decided', 'summary', 'docket']]\n",
    "all_cleaned_df = pd.merge(all_cleaned_captions, otherinfo_tomerge, on = 'docket', how = 'left')\n",
    "\n",
    "## save as pickle\n",
    "import pickle\n",
    "pd.to_pickle(all_cleaned_df, \"/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/cleaned_case_meta.p\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4: clean dates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert from string to datetime\n",
    "all_cases_df['decided_datetime'] = pd.to_datetime(all_cases_df.decided, yearfirst= True)\n",
    "all_cases_df['year_decided'] = all_cases_df.decided_datetime.dt.year \n",
    "all_cases_df['month_decided'] = all_cases_df.decided_datetime.dt.month\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load updated text data\n",
    "new_fulltext = pd.read_pickle('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/all_cases_df_1218_updates.p')\n",
    "all_cases_df = pd.read_pickle('/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/cleaned_case_meta.p')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge with other case information using left join\n",
    "new_fulltext_and_docket = new_fulltext[['docket', 'full_text']]\n",
    "select_columns = [column for column in all_cases_df.columns if column != 'full_text']\n",
    "all_cases_df_newtext = pd.merge(all_cases_df[select_columns], \n",
    "                               new_fulltext_and_docket, on = \"docket\",\n",
    "                               how = \"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/raj2/Dropbox/dph_hearing_decisions/code'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get number of words in each\n",
    "all_case_text = all_cases_df_newtext.full_text.tolist()\n",
    "n_words = [len(i.split()) for i in all_case_text]\n",
    "all_cases_df_newtext['n_words'] = n_words\n",
    "\n",
    "## write the text to intermediate\n",
    "all_cases_df_newtext.to_csv(\"/Users/raj2/Dropbox/dph_hearing_decisions/data/newjersey/intermediate/cases_fulltextdataframe_20181218.csv\", \n",
    "                            index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three: model and summarize (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step two: topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    629\n",
       "1    320\n",
       "Name: emergent_relief, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergent_relief</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3843.349762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2394.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     n_words\n",
       "emergent_relief             \n",
       "0                3843.349762\n",
       "1                2394.031250"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/Users/raj2/nltk_data/corpora/stopwords/english'>\n",
      "  return concat([self.open(f).read() for f in fileids])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'board',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'district',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'education',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'iep',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'parent',\n",
       " 'parents',\n",
       " 'petition',\n",
       " 'petitioner',\n",
       " 'petitioners',\n",
       " 're',\n",
       " 'relief',\n",
       " 'request',\n",
       " 'respondent',\n",
       " 's',\n",
       " 'same',\n",
       " 'school',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'special',\n",
       " 'student',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'would',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '{',\n",
       " '}'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add an indicator for whether it contains the phrase emergent relief\n",
    "all_cases_df_newtext['emergent_relief'] = np.where(all_cases_df_newtext.full_text.str.contains('emergent relief'), \n",
    "                                                  1, 0)\n",
    "all_cases_df_newtext.emergent_relief.value_counts()\n",
    "\n",
    "## look at mean number of words by whether it's emergent relief\n",
    "mean_words = all_cases_df_newtext[['n_words', 'emergent_relief']].groupby(['emergent_relief']).agg('mean')\n",
    "mean_words\n",
    "\n",
    "## plot the difference\n",
    "stop_words = set(stopwords.words('english'))\n",
    "## add punctuation and some application-specific words\n",
    "## to stopword list\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', \n",
    "                   '(', ')', '[', ']', '{', '}', 'education','iep', 'student', 'school', 'district', 'special',\n",
    "                  'parent', 'parents', 'petitioner', 'petitioners', 'petition', 'relief', 'respondent', 'board', 'request',\n",
    "                  'would']) # \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to estimate LDA with certain # of topics\n",
    "def clean_text_estimate_lda(text_list, n_topics):\n",
    "    \n",
    "    ## clean text \n",
    "    \n",
    "    all_stemmed_text = []\n",
    "    for i in range(0, len(text_list)):\n",
    "        one_case_orig = text_list[i]\n",
    "        one_case_orig_token = wordpunct_tokenize(one_case_orig)\n",
    "        one_case_orig_token_lower = [token.lower() for token in one_case_orig_token]\n",
    "        one_case_orig_ns = [i for i in one_case_orig_token_lower if i not in stop_words and len(i) > 3 and i.isalpha()]\n",
    "        one_case_stemmed = [porter.stem(i) for i in one_case_orig_ns]\n",
    "        all_stemmed_text.append(one_case_stemmed)\n",
    "        \n",
    "    ## estimate LDA\n",
    "    dictionary = corpora.Dictionary(all_stemmed_text)\n",
    "    corpus = [dictionary.doc2bow(text) for text in all_stemmed_text]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = n_topics, id2word=dictionary, passes=10,\n",
    "                                          alpha = 'auto',\n",
    "                                          per_word_topics = True)\n",
    "    return(ldamodel)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n"
     ]
    }
   ],
   "source": [
    "## estimate with:\n",
    "## > 100 words\n",
    "## separately for parents and districts\n",
    "## iterate through number of topics: 5, 10, 20, 30\n",
    "district_defendant = all_cases_df_newtext.full_text[(all_cases_df_newtext.final_plaintiff == \"parent\") &\n",
    "                                                   (all_cases_df_newtext.n_words > 100)].tolist()\n",
    "parent_defendant = all_cases_df_newtext.full_text[(all_cases_df_newtext.final_plaintiff != \"parent\") & \n",
    "                                                 (all_cases_df_newtext.n_words > 100)].tolist()\n",
    "\n",
    "## run and iterate over all topics\n",
    "n_topics = [5, 10, 15, 30]\n",
    "store_district_models = []\n",
    "store_parent_models = []\n",
    "for n in n_topics:\n",
    "    one_parent_model = clean_text_estimate_lda(parent_defendant, n_topics = n)\n",
    "    one_district_model = clean_text_estimate_lda(district_defendant, n_topics = n)\n",
    "    store_district_models.append(one_district_model)\n",
    "    store_parent_models.append(one_parent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=6251, num_topics=5, decay=0.5, chunksize=2000)\n",
      "(0, '0.016*\"evalu\" + 0.010*\"child\" + 0.008*\"order\" + 0.008*\"placement\" + 0.008*\"program\" + 0.008*\"hear\" + 0.007*\"servic\" + 0.007*\"decis\" + 0.006*\"educ\" + 0.006*\"state\"')\n",
      "(1, '0.016*\"behavior\" + 0.012*\"program\" + 0.009*\"teacher\" + 0.009*\"placement\" + 0.008*\"provid\" + 0.008*\"appropri\" + 0.007*\"classroom\" + 0.007*\"class\" + 0.007*\"educ\" + 0.007*\"servic\"')\n",
      "(2, '0.017*\"brookfield\" + 0.017*\"threat\" + 0.013*\"drug\" + 0.010*\"placement\" + 0.010*\"distribut\" + 0.010*\"staff\" + 0.009*\"behavior\" + 0.008*\"alleg\" + 0.008*\"novemb\" + 0.007*\"kiernan\"')\n",
      "(3, '0.009*\"child\" + 0.008*\"class\" + 0.008*\"placement\" + 0.007*\"need\" + 0.007*\"teacher\" + 0.006*\"bassman\" + 0.006*\"kitler\" + 0.006*\"student\" + 0.006*\"classroom\" + 0.005*\"program\"')\n",
      "(4, '0.011*\"evalu\" + 0.009*\"provid\" + 0.009*\"educ\" + 0.009*\"author\" + 0.008*\"program\" + 0.007*\"servic\" + 0.006*\"read\" + 0.006*\"disabl\" + 0.006*\"order\" + 0.006*\"appropri\"')\n",
      "LdaModel(num_terms=6251, num_topics=10, decay=0.5, chunksize=2000)\n",
      "(0, '0.018*\"brookfield\" + 0.018*\"threat\" + 0.014*\"drug\" + 0.011*\"distribut\" + 0.010*\"staff\" + 0.010*\"placement\" + 0.009*\"behavior\" + 0.009*\"alleg\" + 0.008*\"novemb\" + 0.008*\"kiernan\"')\n",
      "(1, '0.010*\"class\" + 0.009*\"program\" + 0.009*\"also\" + 0.008*\"time\" + 0.007*\"child\" + 0.007*\"student\" + 0.007*\"teacher\" + 0.007*\"behavior\" + 0.007*\"year\" + 0.006*\"provid\"')\n",
      "(2, '0.013*\"behavior\" + 0.013*\"placement\" + 0.007*\"educ\" + 0.007*\"child\" + 0.006*\"program\" + 0.006*\"disabl\" + 0.006*\"servic\" + 0.006*\"order\" + 0.005*\"provid\" + 0.005*\"state\"')\n",
      "(3, '0.039*\"author\" + 0.011*\"teacher\" + 0.009*\"evalu\" + 0.009*\"provid\" + 0.008*\"report\" + 0.008*\"servic\" + 0.007*\"letter\" + 0.007*\"appear\" + 0.006*\"date\" + 0.006*\"includ\"')\n",
      "(4, '0.012*\"file\" + 0.012*\"limit\" + 0.011*\"reimburs\" + 0.010*\"action\" + 0.010*\"court\" + 0.008*\"hear\" + 0.008*\"claim\" + 0.007*\"issu\" + 0.007*\"assert\" + 0.007*\"decemb\"')\n",
      "(5, '0.015*\"placement\" + 0.011*\"program\" + 0.010*\"order\" + 0.009*\"evalu\" + 0.008*\"appropri\" + 0.008*\"decis\" + 0.008*\"provid\" + 0.007*\"servic\" + 0.007*\"state\" + 0.007*\"hear\"')\n",
      "(6, '0.019*\"evalu\" + 0.010*\"child\" + 0.009*\"educ\" + 0.009*\"program\" + 0.008*\"team\" + 0.008*\"need\" + 0.008*\"servic\" + 0.007*\"grade\" + 0.007*\"disabl\" + 0.006*\"read\"')\n",
      "(7, '0.014*\"evalu\" + 0.011*\"disclosur\" + 0.010*\"date\" + 0.009*\"record\" + 0.008*\"team\" + 0.008*\"march\" + 0.008*\"inform\" + 0.007*\"regard\" + 0.006*\"abll\" + 0.005*\"report\"')\n",
      "(8, '0.023*\"evalu\" + 0.014*\"child\" + 0.011*\"order\" + 0.010*\"hear\" + 0.009*\"decis\" + 0.008*\"time\" + 0.007*\"matter\" + 0.007*\"jersey\" + 0.007*\"offic\" + 0.006*\"state\"')\n",
      "(9, '0.019*\"behavior\" + 0.012*\"program\" + 0.010*\"teacher\" + 0.009*\"class\" + 0.009*\"classroom\" + 0.009*\"placement\" + 0.009*\"need\" + 0.008*\"appropri\" + 0.007*\"provid\" + 0.007*\"regular\"')\n",
      "LdaModel(num_terms=6251, num_topics=15, decay=0.5, chunksize=2000)\n",
      "(0, '0.002*\"behavior\" + 0.002*\"placement\" + 0.002*\"evalu\" + 0.002*\"disabl\" + 0.002*\"appropri\" + 0.001*\"provid\" + 0.001*\"educ\" + 0.001*\"servic\" + 0.001*\"child\" + 0.001*\"teacher\"')\n",
      "(1, '0.019*\"placement\" + 0.015*\"behavior\" + 0.008*\"educ\" + 0.007*\"appropri\" + 0.007*\"program\" + 0.007*\"provid\" + 0.006*\"set\" + 0.006*\"child\" + 0.006*\"instruct\" + 0.005*\"meet\"')\n",
      "(2, '0.014*\"behavior\" + 0.012*\"placement\" + 0.011*\"program\" + 0.009*\"hear\" + 0.008*\"appropri\" + 0.007*\"evalu\" + 0.007*\"provid\" + 0.007*\"servic\" + 0.006*\"educ\" + 0.006*\"order\"')\n",
      "(3, '0.011*\"moot\" + 0.011*\"case\" + 0.009*\"evalu\" + 0.009*\"child\" + 0.008*\"court\" + 0.007*\"matter\" + 0.007*\"public\" + 0.006*\"issu\" + 0.006*\"determin\" + 0.006*\"jersey\"')\n",
      "(4, '0.010*\"teacher\" + 0.009*\"class\" + 0.009*\"program\" + 0.008*\"behavior\" + 0.008*\"year\" + 0.008*\"child\" + 0.008*\"time\" + 0.008*\"need\" + 0.007*\"also\" + 0.007*\"work\"')\n",
      "(5, '0.017*\"report\" + 0.015*\"kitler\" + 0.015*\"bassman\" + 0.013*\"teacher\" + 0.012*\"class\" + 0.010*\"behavior\" + 0.009*\"work\" + 0.007*\"need\" + 0.007*\"comment\" + 0.007*\"judg\"')\n",
      "(6, '0.012*\"servic\" + 0.010*\"placement\" + 0.010*\"bear\" + 0.010*\"tavern\" + 0.009*\"gate\" + 0.009*\"toll\" + 0.009*\"appropri\" + 0.008*\"place\" + 0.008*\"counti\" + 0.007*\"mother\"')\n",
      "(7, '0.017*\"evalu\" + 0.008*\"program\" + 0.008*\"child\" + 0.008*\"educ\" + 0.008*\"team\" + 0.008*\"servic\" + 0.007*\"provid\" + 0.007*\"disabl\" + 0.006*\"languag\" + 0.006*\"read\"')\n",
      "(8, '0.015*\"program\" + 0.010*\"also\" + 0.008*\"student\" + 0.007*\"testifi\" + 0.007*\"placement\" + 0.007*\"progress\" + 0.006*\"team\" + 0.006*\"teacher\" + 0.006*\"provid\" + 0.006*\"child\"')\n",
      "(9, '0.019*\"evalu\" + 0.017*\"order\" + 0.017*\"emerg\" + 0.009*\"decis\" + 0.009*\"parti\" + 0.009*\"child\" + 0.009*\"matter\" + 0.008*\"shall\" + 0.007*\"offic\" + 0.007*\"applic\"')\n",
      "(10, '0.014*\"evalu\" + 0.014*\"assess\" + 0.009*\"conduct\" + 0.008*\"decis\" + 0.008*\"order\" + 0.008*\"reevalu\" + 0.008*\"placement\" + 0.007*\"grant\" + 0.007*\"educ\" + 0.007*\"parti\"')\n",
      "(11, '0.034*\"berlin\" + 0.014*\"ideia\" + 0.014*\"borough\" + 0.011*\"hewitt\" + 0.010*\"placement\" + 0.005*\"open\" + 0.005*\"rational\" + 0.005*\"brookfield\" + 0.005*\"staff\" + 0.004*\"township\"')\n",
      "(12, '0.039*\"author\" + 0.014*\"evalu\" + 0.010*\"teacher\" + 0.008*\"provid\" + 0.007*\"servic\" + 0.007*\"appear\" + 0.006*\"report\" + 0.006*\"includ\" + 0.006*\"process\" + 0.006*\"time\"')\n",
      "(13, '0.019*\"evalu\" + 0.015*\"child\" + 0.010*\"educ\" + 0.010*\"servic\" + 0.009*\"placement\" + 0.009*\"decis\" + 0.009*\"hear\" + 0.009*\"provid\" + 0.009*\"program\" + 0.008*\"disabl\"')\n",
      "(14, '0.018*\"brookfield\" + 0.018*\"threat\" + 0.013*\"drug\" + 0.011*\"distribut\" + 0.010*\"staff\" + 0.010*\"placement\" + 0.009*\"behavior\" + 0.009*\"alleg\" + 0.008*\"novemb\" + 0.008*\"kiernan\"')\n",
      "LdaModel(num_terms=6251, num_topics=30, decay=0.5, chunksize=2000)\n",
      "(17, '0.001*\"behavior\" + 0.001*\"placement\" + 0.001*\"class\" + 0.001*\"program\" + 0.001*\"need\" + 0.001*\"appropri\" + 0.001*\"educ\" + 0.001*\"teacher\" + 0.001*\"child\" + 0.001*\"disabl\"')\n",
      "(1, '0.145*\"author\" + 0.017*\"monilla\" + 0.016*\"provid\" + 0.015*\"auditori\" + 0.015*\"appear\" + 0.012*\"wrote\" + 0.011*\"system\" + 0.011*\"alleg\" + 0.010*\"process\" + 0.010*\"servic\"')\n",
      "(10, '0.001*\"placement\" + 0.001*\"servic\" + 0.001*\"program\" + 0.001*\"behavior\" + 0.001*\"year\" + 0.001*\"child\" + 0.001*\"provid\" + 0.001*\"evalu\" + 0.001*\"appropri\" + 0.001*\"educ\"')\n",
      "(14, '0.007*\"michael\" + 0.005*\"ravin\" + 0.004*\"cynthia\" + 0.004*\"levi\" + 0.003*\"arsen\" + 0.003*\"zartarian\" + 0.003*\"associ\" + 0.002*\"blackburn\" + 0.001*\"program\" + 0.001*\"decis\"')\n",
      "(7, '0.003*\"placement\" + 0.002*\"evalu\" + 0.002*\"hear\" + 0.002*\"order\" + 0.002*\"servic\" + 0.002*\"disabl\" + 0.001*\"child\" + 0.001*\"provid\" + 0.001*\"program\" + 0.001*\"instruct\"')\n",
      "(2, '0.020*\"servic\" + 0.019*\"counti\" + 0.015*\"mother\" + 0.015*\"gloucest\" + 0.013*\"place\" + 0.011*\"class\" + 0.010*\"teacher\" + 0.009*\"classroom\" + 0.009*\"appropri\" + 0.008*\"evalu\"')\n",
      "(18, '0.041*\"evalu\" + 0.022*\"independ\" + 0.016*\"process\" + 0.014*\"report\" + 0.014*\"file\" + 0.010*\"indic\" + 0.009*\"time\" + 0.009*\"request\" + 0.008*\"result\" + 0.008*\"dailey\"')\n",
      "(22, '0.026*\"behavior\" + 0.021*\"manifest\" + 0.020*\"disabl\" + 0.017*\"determin\" + 0.015*\"child\" + 0.013*\"altern\" + 0.012*\"meet\" + 0.012*\"tyrrel\" + 0.012*\"team\" + 0.011*\"day\"')\n",
      "(8, '0.026*\"boy\" + 0.013*\"son\" + 0.003*\"hundr\" + 0.003*\"animu\" + 0.003*\"sheaf\" + 0.003*\"thick\" + 0.003*\"placement\" + 0.003*\"staff\" + 0.002*\"educ\" + 0.002*\"brookfield\"')\n",
      "(0, '0.011*\"broad\" + 0.010*\"bader\" + 0.007*\"attison\" + 0.007*\"compound\" + 0.007*\"ginsberg\" + 0.006*\"march\" + 0.005*\"richard\" + 0.004*\"dumont\" + 0.004*\"shapiro\" + 0.004*\"mcgill\"')\n",
      "(28, '0.018*\"brookfield\" + 0.018*\"threat\" + 0.014*\"drug\" + 0.011*\"distribut\" + 0.010*\"staff\" + 0.010*\"placement\" + 0.009*\"behavior\" + 0.009*\"alleg\" + 0.009*\"novemb\" + 0.008*\"kiernan\"')\n",
      "(3, '0.023*\"evalu\" + 0.016*\"need\" + 0.012*\"program\" + 0.012*\"provid\" + 0.011*\"educ\" + 0.011*\"servic\" + 0.009*\"order\" + 0.008*\"appropri\" + 0.008*\"teacher\" + 0.007*\"consent\"')\n",
      "(4, '0.012*\"teacher\" + 0.012*\"program\" + 0.011*\"evalu\" + 0.010*\"report\" + 0.010*\"exhibit\" + 0.009*\"need\" + 0.009*\"shapiro\" + 0.009*\"also\" + 0.009*\"nobl\" + 0.008*\"wilson\"')\n",
      "(23, '0.013*\"evalu\" + 0.010*\"emerg\" + 0.010*\"program\" + 0.009*\"placement\" + 0.009*\"decis\" + 0.008*\"hear\" + 0.008*\"order\" + 0.008*\"shall\" + 0.008*\"matter\" + 0.008*\"attend\"')\n",
      "(9, '0.016*\"provid\" + 0.016*\"program\" + 0.015*\"educ\" + 0.012*\"servic\" + 0.011*\"placement\" + 0.011*\"child\" + 0.011*\"appropri\" + 0.011*\"disabl\" + 0.010*\"regular\" + 0.009*\"hear\"')\n",
      "(27, '0.030*\"emerg\" + 0.017*\"order\" + 0.014*\"grant\" + 0.014*\"harm\" + 0.013*\"child\" + 0.011*\"applic\" + 0.010*\"suffer\" + 0.010*\"request\" + 0.009*\"parti\" + 0.009*\"find\"')\n",
      "(21, '0.019*\"behavior\" + 0.015*\"program\" + 0.011*\"placement\" + 0.010*\"appropri\" + 0.010*\"provid\" + 0.009*\"educ\" + 0.008*\"teacher\" + 0.007*\"year\" + 0.007*\"class\" + 0.006*\"aggress\"')\n",
      "(29, '0.011*\"behavior\" + 0.008*\"class\" + 0.008*\"teacher\" + 0.007*\"time\" + 0.006*\"year\" + 0.006*\"program\" + 0.006*\"evalu\" + 0.006*\"grade\" + 0.006*\"also\" + 0.006*\"need\"')\n",
      "(11, '0.020*\"placement\" + 0.010*\"program\" + 0.010*\"educ\" + 0.010*\"order\" + 0.009*\"evalu\" + 0.009*\"child\" + 0.009*\"appropri\" + 0.008*\"matter\" + 0.008*\"provid\" + 0.007*\"hear\"')\n",
      "(26, '0.028*\"evalu\" + 0.012*\"child\" + 0.011*\"decis\" + 0.010*\"order\" + 0.009*\"servic\" + 0.009*\"hear\" + 0.008*\"assess\" + 0.008*\"jersey\" + 0.008*\"offic\" + 0.008*\"team\"')\n"
     ]
    }
   ],
   "source": [
    "## export for use in STM in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.008*\"jersey\" + 0.008*\"matter\" + 0.008*\"decis\" + 0.008*\"hear\" + 0.008*\"parent\" + 0.007*\"placement\" + 0.007*\"date\" + 0.007*\"order\" + 0.006*\"program\" + 0.006*\"state\"')\n",
      "(1, '0.019*\"program\" + 0.008*\"provid\" + 0.008*\"class\" + 0.007*\"year\" + 0.007*\"behavior\" + 0.007*\"teacher\" + 0.007*\"also\" + 0.007*\"skill\" + 0.006*\"appropri\" + 0.006*\"time\"')\n",
      "(2, '0.011*\"grade\" + 0.009*\"year\" + 0.008*\"read\" + 0.007*\"class\" + 0.007*\"teacher\" + 0.007*\"parent\" + 0.007*\"evalu\" + 0.006*\"time\" + 0.006*\"report\" + 0.006*\"program\"')\n",
      "(3, '0.015*\"provid\" + 0.011*\"program\" + 0.011*\"servic\" + 0.009*\"child\" + 0.009*\"educ\" + 0.008*\"placement\" + 0.007*\"requir\" + 0.007*\"parent\" + 0.007*\"state\" + 0.007*\"appropri\"')\n",
      "(4, '0.009*\"date\" + 0.008*\"state\" + 0.008*\"provid\" + 0.008*\"servic\" + 0.007*\"placement\" + 0.007*\"educ\" + 0.006*\"meet\" + 0.006*\"child\" + 0.006*\"sinai\" + 0.006*\"parent\"')\n"
     ]
    }
   ],
   "source": [
    "topics = test.print_topics(num_words = 10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.010*\"program\" + 0.009*\"placement\" + 0.008*\"home\" + 0.007*\"emerg\" + 0.007*\"date\" + 0.006*\"provid\" + 0.006*\"jersey\" + 0.006*\"matter\" + 0.006*\"parent\" + 0.006*\"year\" + 0.006*\"time\" + 0.006*\"order\" + 0.005*\"servic\" + 0.005*\"state\" + 0.005*\"child\" + 0.005*\"decis\" + 0.005*\"hear\" + 0.004*\"case\" + 0.004*\"offic\" + 0.004*\"parti\"')\n",
      "(1, '0.009*\"educ\" + 0.008*\"placement\" + 0.008*\"state\" + 0.007*\"provid\" + 0.006*\"program\" + 0.006*\"behavior\" + 0.006*\"year\" + 0.006*\"class\" + 0.006*\"servic\" + 0.006*\"disabl\" + 0.005*\"appropri\" + 0.005*\"child\" + 0.005*\"grade\" + 0.005*\"read\" + 0.005*\"requir\" + 0.004*\"meet\" + 0.004*\"student\" + 0.004*\"need\" + 0.004*\"also\" + 0.004*\"evalu\"')\n",
      "(2, '0.012*\"evalu\" + 0.012*\"provid\" + 0.010*\"child\" + 0.010*\"program\" + 0.010*\"parent\" + 0.010*\"servic\" + 0.009*\"educ\" + 0.008*\"placement\" + 0.008*\"appropri\" + 0.007*\"year\" + 0.007*\"disabl\" + 0.006*\"hear\" + 0.006*\"decis\" + 0.006*\"state\" + 0.006*\"requir\" + 0.006*\"meet\" + 0.005*\"need\" + 0.005*\"determin\" + 0.005*\"matter\" + 0.005*\"team\"')\n",
      "(3, '0.019*\"program\" + 0.009*\"provid\" + 0.009*\"behavior\" + 0.007*\"year\" + 0.007*\"also\" + 0.007*\"skill\" + 0.007*\"servic\" + 0.006*\"appropri\" + 0.006*\"class\" + 0.006*\"time\" + 0.006*\"educ\" + 0.006*\"need\" + 0.006*\"children\" + 0.006*\"teacher\" + 0.006*\"child\" + 0.006*\"languag\" + 0.005*\"placement\" + 0.005*\"classroom\" + 0.005*\"evalu\" + 0.005*\"progress\"')\n",
      "(4, '0.011*\"grade\" + 0.010*\"class\" + 0.009*\"read\" + 0.009*\"teacher\" + 0.007*\"testifi\" + 0.007*\"year\" + 0.007*\"also\" + 0.007*\"time\" + 0.007*\"parent\" + 0.006*\"report\" + 0.006*\"program\" + 0.006*\"student\" + 0.006*\"test\" + 0.005*\"need\" + 0.005*\"meet\" + 0.005*\"provid\" + 0.004*\"evalu\" + 0.004*\"learn\" + 0.004*\"work\" + 0.004*\"placement\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel.save('model_10topics_new1218.gensim')\n",
    "topics = ldamodel.print_topics(num_words = 20)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "### visualize\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py:68: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\n",
      "  return matrix(data, dtype=dtype, copy=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-1cafa235f1ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    396\u001b[0m    \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m    top_terms = pd.concat(Parallel(n_jobs=n_jobs)(delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls) \\\n\u001b[0;32m--> 255\u001b[0;31m                                                  for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[1;32m    256\u001b[0m    \u001b[0mtopic_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
